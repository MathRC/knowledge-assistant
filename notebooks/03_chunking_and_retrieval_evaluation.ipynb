{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a89097-1e66-4b99-a9ed-960a91d2dd6e",
   "metadata": {},
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "In the previous notebooks, I processed a complex PDF, created two sets of text chunks in order to test two different chunking strategies, generated vector embeddings, and indexed everything in a Weaviate database. Now, we've reached a crucial stage: **retrieval**. How well can we find the right information to answer a user's query?\n",
    "\n",
    "In this notebook, I run two key experiments to answer that question:\n",
    "\n",
    "1.  **Chunking Strategy Comparison**: I'll test `fixed-size` chunks against `section-based` chunks to see which strategy yields more relevant context.\n",
    "2.  **Search Method Comparison**: I'll compare the performance of different search algorithms—keyword (BM25), pure vector (semantic), and hybrid search—to see which performs best for this use case.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ecc269-3d11-41fa-aa10-ecf500236742",
   "metadata": {},
   "source": [
    "## **2. The Ground Truth Dataset**\n",
    "\n",
    "To provide an objective benchmark for these experiments, I created a custom **ground truth dataset**, which is available in the project at `data/evaluation/ground_truth.json`.\n",
    "\n",
    "This file contains a list of questions a translator might realistically ask. For each question, I manually identified the exact passages from the style guide that should be retrieved to provide a comprehensive answer. Each of these expected passages is also assigned a relevance level ('Primary', 'Secondary', or 'Tertiary'), allowing for a more nuanced, weighted evaluation.\n",
    "\n",
    "This dataset acts as our \"golden set.\" By comparing the retrieved chunks from each strategy against these ideal answers, we can quantitatively measure performance using metrics like F1-score and make data-driven decisions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824fb521-a7c7-4c86-b670-e77f4f695f42",
   "metadata": {},
   "source": [
    "## **3. Setup and Configuration**\n",
    "\n",
    "This first cell imports the necessary libraries and sets up the key constants for our evaluation experiments.\n",
    "\n",
    "* **Libraries**:\n",
    "    * `weaviate` and `weaviate.classes.query as wq`: These are used to connect to our database and construct the different types of search queries we want to test.\n",
    "    * `sentence_transformers` & `torch`: We need these again to load the *same* `BGE` embedding model. To perform a vector search, the user's question must be converted into a vector using the exact same model that was used to embed the documents.\n",
    "    * `json`, `os`, `datetime`: Standard libraries for loading our ground truth evaluation file and for saving the timestamped results of our experiments.\n",
    "* **Constants**:\n",
    "    * `COLLECTION_NAME`: Specifies the `StyleGuide` collection in Weaviate that we will be querying.\n",
    "    * `MODEL_NAME`: Ensures we load the correct embedding model to create vectors for our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c162c674-2d26-411e-9618-83df1390da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS AND CONFIGURATION ===\n",
    "import weaviate\n",
    "import weaviate.classes.query as wq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Project Constants ---\n",
    "COLLECTION_NAME = 'StyleGuide'\n",
    "MODEL_NAME = 'BAAI/bge-large-en-v1.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b996837c-ed43-47a6-85db-e4ed7bb7a376",
   "metadata": {},
   "source": [
    "The second cell prepares the environment for the evaluation experiments. It performs three key actions:\n",
    "1.  **Checks for a GPU** to ensure the embedding process for our queries runs as fast as possible.\n",
    "2.  **Loads the `BGE` embedding model** into memory, making it ready to convert questions into vectors.\n",
    "3.  **Connects to the Weaviate database** and accesses the `StyleGuide` collection, so we can start running queries against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8bf976-9927-424e-8f8b-3425ce76c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Model and Connecting to Weaviate ---\n",
      "✅ CUDA available. Using GPU: NVIDIA GeForce GTX 1050 Ti\n",
      "Loading embedding model 'BAAI/bge-large-en-v1.5'...\n",
      "✅ Embedding model loaded successfully\n",
      "✅ Connected to Weaviate collection 'StyleGuide'.\n"
     ]
    }
   ],
   "source": [
    "# === INITIALIZE MODEL AND CONNECT TO WEAVIATE ===\n",
    "print('--- Initializing Model and Connecting to Weaviate ---')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f'✅ CUDA available. Using GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('⚠️ CUDA not available. Using CPU.')\n",
    "\n",
    "print(f\"Loading embedding model '{MODEL_NAME}'...\")\n",
    "embedding_model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "print('✅ Embedding model loaded successfully')\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "collection = client.collections.get(COLLECTION_NAME)\n",
    "print(f\"✅ Connected to Weaviate collection '{COLLECTION_NAME}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f3c4c-e50a-4af5-96c2-3dbc94a005cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8fbc1-6fe8-4471-b45c-4057dad1629d",
   "metadata": {},
   "source": [
    "## **4. Defining the Search Functions**\n",
    "\n",
    "Now that the environment is ready, the next step is to define the functions that will perform the actual searches. To find the most effective retrieval method, I've implemented three different strategies supported by Weaviate:\n",
    "\n",
    "1.  **Pure Vector Search (`query_semantic_vector`)**: This function takes a question, converts it into a vector embedding, and searches for the chunks with the most similar vectors in the database based on cosine similarity. This method is excellent at finding semantically related content, even if the keywords don't match exactly.\n",
    "2.  **Keyword Search (`query_semantic_bm25`)**: This function uses the classic BM25 algorithm, a powerful keyword-based search method. It's very effective when the user's query contains the exact terms present in the source document.\n",
    "3.  **Hybrid Search (`query_semantic_hybrid`)**: This function combines the strengths of both previous methods. It performs a vector search and a keyword search, then intelligently blends the results. The `alpha` parameter controls the balance: `alpha=0` is pure keyword search, `alpha=1` is pure vector search, and values in between create a weighted combination.\n",
    "\n",
    "Each function is also designed to accept a `method_filter`, which allows us to restrict the search to only fixed-size or section-based chunks, a crucial feature for our first experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a951f30e-e42e-4be5-a634-8ea458e68567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEARCH FUNCTIONS ===\n",
    "def query_semantic_vector(question: str, method_filter: str, limit: int = 5):\n",
    "    \"\"\"Performs a pure vector similarity search in Weaviate.\"\"\"\n",
    "    question_vector = embedding_model.encode(question).tolist()\n",
    "    \n",
    "    query_filter = wq.Filter.by_property('method').like(method_filter) if method_filter else None\n",
    "\n",
    "    response = collection.query.near_vector(\n",
    "        near_vector=question_vector,\n",
    "        limit=limit,\n",
    "        filters=query_filter,\n",
    "        return_metadata=wq.MetadataQuery(distance=True),\n",
    "        return_properties=['text', 'part', 'chapter', 'section', 'subsection', 'method', 'chunk_id', 'page_number']\n",
    "    )\n",
    "\n",
    "    if not response.objects:\n",
    "        return []\n",
    "\n",
    "    return [{\n",
    "        'chunk_id': obj.properties['chunk_id'],\n",
    "        'text': obj.properties['text'],\n",
    "        'part': obj.properties.get('part'),\n",
    "        'chapter': obj.properties.get('chapter'),\n",
    "        'section': obj.properties.get('section'),\n",
    "        'subsection': obj.properties.get('subsection'),\n",
    "        'page_number': obj.properties.get('page_number', 'N/A'),\n",
    "        'method': obj.properties.get('method'),\n",
    "        'relevance_score': 1 - obj.metadata.distance,\n",
    "        'search_method': 'Vector'\n",
    "    } for obj in response.objects]\n",
    "\n",
    "def query_semantic_hybrid(question: str, method_filter: str, limit: int = 5, alpha: float = 0.5):\n",
    "    \"\"\"Performs a hybrid search in Weaviate, combining vector and keyword methods.\"\"\"\n",
    "    question_vector = embedding_model.encode(question).tolist()\n",
    "    \n",
    "    query_filter = wq.Filter.by_property('method').like(method_filter) if method_filter else None\n",
    "\n",
    "    response = collection.query.hybrid(\n",
    "        query=question,\n",
    "        vector=question_vector,\n",
    "        alpha=alpha,\n",
    "        limit=limit,\n",
    "        filters=query_filter,\n",
    "        return_metadata=wq.MetadataQuery(score=True),\n",
    "        return_properties=['text', 'part', 'chapter', 'section', 'subsection', 'method', 'chunk_id', 'page_number']\n",
    "    )\n",
    "\n",
    "    if not response.objects:\n",
    "        return []\n",
    "\n",
    "    return [{\n",
    "        'chunk_id': obj.properties['chunk_id'],\n",
    "        'text': obj.properties['text'],\n",
    "        'part': obj.properties.get('part'),\n",
    "        'chapter': obj.properties.get('chapter'),\n",
    "        'section': obj.properties.get('section'),\n",
    "        'subsection': obj.properties.get('subsection'),\n",
    "        'page_number': obj.properties.get('page_number', 'N/A'),\n",
    "        'method': obj.properties.get('method'),\n",
    "        'relevance_score': obj.metadata.score,\n",
    "        'search_method': 'Hybrid'\n",
    "    } for obj in response.objects]\n",
    "\n",
    "def query_semantic_bm25(question: str, method_filter: str, limit: int = 5):\n",
    "    \"\"\"Performs a pure keyword (BM25) search in Weaviate.\"\"\"\n",
    "    query_filter = wq.Filter.by_property('method').like(method_filter) if method_filter else None\n",
    "\n",
    "    response = collection.query.bm25(\n",
    "        query=question,\n",
    "        limit=limit,\n",
    "        filters=query_filter,\n",
    "        return_metadata=wq.MetadataQuery(score=True),\n",
    "        return_properties=['text', 'part', 'chapter', 'section', 'subsection', 'method', 'chunk_id', 'page_number']\n",
    "    )\n",
    "\n",
    "    if not response.objects:\n",
    "        return []\n",
    "\n",
    "    return [{\n",
    "        'chunk_id': obj.properties['chunk_id'],\n",
    "        'text': obj.properties['text'],\n",
    "        'part': obj.properties.get('part'),\n",
    "        'chapter': obj.properties.get('chapter'),\n",
    "        'section': obj.properties.get('section'),\n",
    "        'subsection': obj.properties.get('subsection'),\n",
    "        'page_number': obj.properties.get('page_number', 'N/A'),\n",
    "        'method': obj.properties.get('method'),\n",
    "        'relevance_score': obj.metadata.score,\n",
    "        'search_method': 'BM25'\n",
    "    } for obj in response.objects]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78719a3-d5a6-4e57-a1ec-34d2e11257e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3aaab-e330-4f7b-a54e-4e4340241a39",
   "metadata": {},
   "source": [
    "## **5. Retrieval Evaluation Methodology**\n",
    "\n",
    "After defining the search functions, we need to objectively measure their performance. This section defines the tools for our evaluation: a function to load our ground truth dataset and the functions that will calculate the performance scores.\n",
    "\n",
    "### **5.1. Loading the Ground Truth Data**\n",
    "\n",
    "The `load_ground_truth` function loads our manually created `ground_truth.json` file. This file acts as our \"answer key\" for the experiments, containing the questions we will ask and the exact text passages we expect to be retrieved for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6588d5fb-ba03-461c-94e4-4e162f134ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GROUND TRUTH LOADING ===\n",
    "def load_ground_truth():\n",
    "    \"\"\"Loads ground truth data from the dedicated JSON file.\"\"\"\n",
    "    with open('../data/evaluation/ground_truth.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return {\n",
    "        str(q['id']): {'question': q['question'], 'entries': q['ground_truth']}\n",
    "        for q in data['questions']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe47c6d-40b1-4c71-9cfa-a4f8de02e290",
   "metadata": {},
   "source": [
    "### **5.2. Defining the Performance Metrics**\n",
    "\n",
    "To get a quantitative score for each retrieval strategy, I've implemented three key metric functions:\n",
    "* **`calculate_text_coverage`**: This is effectively our **recall** score. It measures what percentage of the expected answer text was successfully retrieved.\n",
    "* **`calculate_content_precision`**: This measures **precision**. Of all the text that was retrieved, it calculates what percentage was actually relevant.\n",
    "* **`calculate_normalized_score`**: This is the main scoring function that combines coverage and precision into a single **F1-score**. It also uses a weighted system, giving more points for retrieving 'Primary' relevance passages than 'Tertiary' ones. This results in a more nuanced performance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea521a84-44a1-458f-966d-630bd9035bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATION METRIC FUNCTIONS ===\n",
    "def calculate_content_precision(retrieved_results, ground_truth_entries):\n",
    "    \"\"\"Calculates the percentage of retrieved content that is relevant.\"\"\"\n",
    "    if not retrieved_results: return 0.0\n",
    "    \n",
    "    all_retrieved_text = ' '.join([result['text'] for result in retrieved_results])\n",
    "    retrieved_words = set(all_retrieved_text.lower().split())\n",
    "    if not retrieved_words: return 0.0\n",
    "    \n",
    "    all_expected_text = ' '.join([entry['expected_text'] for entry in ground_truth_entries])\n",
    "    expected_words = set(all_expected_text.lower().split())\n",
    "    \n",
    "    relevant_words = retrieved_words & expected_words\n",
    "    return len(relevant_words) / len(retrieved_words)\n",
    "\n",
    "def calculate_text_coverage(retrieved_results, expected_text):\n",
    "    \"\"\"Calculates the percentage of a single expected text that was retrieved.\"\"\"\n",
    "    if not retrieved_results or not expected_text: return 0.0\n",
    "    \n",
    "    retrieved_text = ' '.join([result['text'] for result in retrieved_results]).lower().strip()\n",
    "    expected_text = expected_text.lower().strip()\n",
    "    \n",
    "    expected_words = set(expected_text.split())\n",
    "    retrieved_words = set(retrieved_text.split())\n",
    "    \n",
    "    overlap = len(expected_words & retrieved_words)\n",
    "    coverage = overlap / len(expected_words) if expected_words else 0\n",
    "    return min(coverage, 1.0)\n",
    "\n",
    "def calculate_normalized_score(retrieved_results, ground_truth_entries, k=5):\n",
    "    \"\"\"Calculates composite evaluation metrics (Coverage, Precision, F1).\"\"\"\n",
    "    relevance_scores = {'Primary': 5, 'Secondary': 3, 'Tertiary': 1}\n",
    "    coverage_details = []\n",
    "    total_weighted_coverage = 0\n",
    "    \n",
    "    for gt_entry in ground_truth_entries:\n",
    "        coverage = calculate_text_coverage(retrieved_results, gt_entry['expected_text'])\n",
    "        relevance_weight = relevance_scores[gt_entry['relevance']]\n",
    "        weighted_coverage = coverage * relevance_weight\n",
    "        total_weighted_coverage += weighted_coverage\n",
    "        \n",
    "        coverage_details.append({\n",
    "            'ground_truth_section': gt_entry['section_title'],\n",
    "            'relevance': gt_entry['relevance'],\n",
    "            'coverage': coverage,\n",
    "            'weighted_score': weighted_coverage,\n",
    "            'max_possible': relevance_weight\n",
    "        })\n",
    "    \n",
    "    max_possible_coverage = sum(relevance_scores[entry['relevance']] for entry in ground_truth_entries)\n",
    "    coverage_score = total_weighted_coverage / max_possible_coverage if max_possible_coverage > 0 else 0\n",
    "    content_precision = calculate_content_precision(retrieved_results, ground_truth_entries)\n",
    "    content_f1 = 2 * (content_precision * coverage_score) / (content_precision + coverage_score) if (content_precision + coverage_score) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'coverage_score': coverage_score,\n",
    "        'content_precision': content_precision,\n",
    "        'content_f1': content_f1,\n",
    "        'raw_coverage': total_weighted_coverage,\n",
    "        'max_possible': max_possible_coverage,\n",
    "        'coverage_details': coverage_details\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37130c4-2a67-4b55-b94d-583ce3717e81",
   "metadata": {},
   "source": [
    "### **5.3. Evaluation and Comparison Functions**\n",
    "\n",
    "With our search methods and evaluation metrics defined, these final two functions execute the experiments and compile the results.\n",
    "\n",
    "* The **`evaluate_system`** function is the core of the retrieval evaluation pipeline. It takes a specific search function (e.g., `query_semantic_vector`) as input and runs it against every question in our ground truth dataset. For each question, it calculates the performance metrics we defined earlier and returns a detailed dictionary with the scores and the retrieved chunks.\n",
    "* The **`compare_methods`** function presents the final results. It takes the output from multiple runs of `evaluate_system` and formats it into a clean comparison table, making it easy to see the F1-score for each strategy and the percentage improvement over our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270fa30a-a71a-438d-9bb3-634d2ffd1ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SYSTEM EVALUATION AND COMPARISON FUNCTIONS ===\n",
    "def evaluate_system(search_function, method_name, ground_truth, k=5):\n",
    "    \"\"\"Runs a search function against all ground truth questions and calculates metrics.\"\"\"\n",
    "    results = {'method': method_name, 'question_scores': {}, 'total_questions': len(ground_truth)}\n",
    "    total_f1 = 0\n",
    "\n",
    "    print(f'Evaluating {method_name}...')\n",
    "    print('-' * 50)\n",
    "\n",
    "    for q_id, gt_data in ground_truth.items():\n",
    "        method_filter = 'section_based*' if 'section' in method_name.lower() else 'fixed_size'\n",
    "        search_results = search_function(gt_data['question'], method_filter, k)\n",
    "        metrics = calculate_normalized_score(search_results, gt_data['entries'], k)\n",
    "        total_f1 += metrics['content_f1']\n",
    "\n",
    "        results['question_scores'][q_id] = {\n",
    "            'question': gt_data['question'],\n",
    "            'metrics': metrics,\n",
    "            'retrieved_context': [f\"{r.get('chapter', '')} > {r.get('section', '')}\" for r in search_results],\n",
    "            'retrieved_chunks': search_results\n",
    "        }\n",
    "        print(f\"Q{q_id}: {gt_data['question'][:60]}...\")\n",
    "        print(f\"  F1: {metrics['content_f1']:.3f}\")\n",
    "    \n",
    "    results['overall_metrics'] = {'avg_content_f1': total_f1 / len(ground_truth)}\n",
    "    print(f\"\\nOverall Average F1: {results['overall_metrics']['avg_content_f1']:.3f}\")\n",
    "    print('=' * 50, '\\n')\n",
    "    return results\n",
    "\n",
    "def compare_methods(results_list):\n",
    "    \"\"\"Displays a comparison table of evaluation results for different methods.\"\"\"\n",
    "    print('=== METHOD COMPARISON ===')\n",
    "    print(f\"{'Method':<40} {'F1 Score':<10} {'Improvement'}\")\n",
    "    print('-' * 70)\n",
    "    \n",
    "    baseline_f1 = results_list[0]['overall_metrics']['avg_content_f1']\n",
    "    for i, results in enumerate(results_list):\n",
    "        f1_score = results['overall_metrics']['avg_content_f1']\n",
    "        improvement = f'{((f1_score - baseline_f1) / baseline_f1 * 100):+.1f}%' if i > 0 else 'Baseline'\n",
    "        print(f\"{results['method']:<40} {f1_score:<10.3f} {improvement}\")\n",
    "    \n",
    "    print('-' * 70, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f03a7b-706b-4de1-9acb-d5d2456a658d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5bd94-7622-434a-bb35-8af44a2e8e17",
   "metadata": {},
   "source": [
    "## **6. Evaluation of Chunking Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb8150-d5da-415a-85b5-765ddc80d507",
   "metadata": {},
   "source": [
    "Now that the evaluation framework is ready, the final step before running the experiments is to load our ground truth dataset. The cell below handles this, loading the questions and the \"answer key\" that we will use to score our retrieval methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e55c0e87-8f38-442e-b532-84a0f6d7ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth data...\n",
      "Loaded 5 questions with ground truth labels.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# === LOAD GROUND TRUTH DATA ===\n",
    "print('Loading ground truth data...')\n",
    "ground_truth = load_ground_truth()\n",
    "print(f'Loaded {len(ground_truth)} questions with ground truth labels.')\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840803af-bd7c-404f-a49a-42ddbb3b2aee",
   "metadata": {},
   "source": [
    "With the ground truth data loaded, we can now run our first experiment. The goal is to answer our first key question: **What is the impact of different chunking strategies on retrieval quality?**\n",
    "\n",
    "The code below evaluates both the `fixed-size` and `section-based` strategies. To ensure a fair comparison, both will be tested using the same **pure vector search** method. This isolates the chunking strategy as the only variable. The `fixed-size` method will serve as our baseline, against which we'll measure any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c26c03ae-ea95-4356-9eac-c9a11616aa5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPERIMENT 1: CHUNKING STRATEGY COMPARISON\n",
      "Testing: Fixed-size vs Section-based chunks (both using Vector search)\n",
      "================================================================================\n",
      "Evaluating Fixed-size + Vector...\n",
      "--------------------------------------------------\n",
      "Q1: When is it correct to use square brackets?...\n",
      "  F1: 0.637\n",
      "Q2: What’s the proper usage of an en dash?...\n",
      "  F1: 0.662\n",
      "Q3: Which spelling is correct: Rhein or Rhin?...\n",
      "  F1: 0.322\n",
      "Q4: Is it better to use the symbol '*' or 'x' for multiplication...\n",
      "  F1: 0.558\n",
      "Q5: Is there a space before or after suspension points?...\n",
      "  F1: 0.490\n",
      "\n",
      "Overall Average F1: 0.534\n",
      "================================================== \n",
      "\n",
      "Evaluating Section-based + Vector...\n",
      "--------------------------------------------------\n",
      "Q1: When is it correct to use square brackets?...\n",
      "  F1: 0.781\n",
      "Q2: What’s the proper usage of an en dash?...\n",
      "  F1: 0.705\n",
      "Q3: Which spelling is correct: Rhein or Rhin?...\n",
      "  F1: 0.399\n",
      "Q4: Is it better to use the symbol '*' or 'x' for multiplication...\n",
      "  F1: 0.868\n",
      "Q5: Is there a space before or after suspension points?...\n",
      "  F1: 0.799\n",
      "\n",
      "Overall Average F1: 0.710\n",
      "================================================== \n",
      "\n",
      "=== METHOD COMPARISON ===\n",
      "Method                                   F1 Score   Improvement\n",
      "----------------------------------------------------------------------\n",
      "Fixed-size + Vector                      0.534      Baseline\n",
      "Section-based + Vector                   0.710      +33.0%\n",
      "---------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === EXPERIMENT 1: CHUNKING STRATEGY COMPARISON ===\n",
    "print('\\nEXPERIMENT 1: CHUNKING STRATEGY COMPARISON')\n",
    "print('Testing: Fixed-size vs Section-based chunks (both using Vector search)')\n",
    "print('='*80)\n",
    "\n",
    "results_fixed = evaluate_system(query_semantic_vector, 'Fixed-size + Vector', ground_truth)\n",
    "results_section = evaluate_system(query_semantic_vector, 'Section-based + Vector', ground_truth)\n",
    "\n",
    "compare_methods([results_fixed, results_section])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea940264-c63c-4c7d-a297-7d6655e45ec3",
   "metadata": {},
   "source": [
    "### **6.1. Chunking Strategies: Analysis of Results**\n",
    "\n",
    "The results are very clear: the **`section-based`** chunking strategy significantly outperforms the `fixed-size` baseline, achieving an average F1-score of **0.710** compared to **0.534**. That is a **33% improvement**.\n",
    "\n",
    "This outcome suggests that preserving the document's logical structure is critical for retrieval quality. By creating chunks that align with the document's chapters and sections, we generate more contextually complete and relevant units of information. The `fixed-size` approach, while simpler and faster, often retrieves fragments that contain both relevant and irrelevant text, which hurts its precision and overall F1-score.\n",
    "\n",
    "Based on this result, **we will use the `section-based` chunks for all subsequent experiments** in this project.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c302b-a031-40c2-b9d2-d18897dd1e92",
   "metadata": {},
   "source": [
    "## **7. Evaluation of Search Methods**\n",
    "\n",
    "Now that we've confirmed that `section-based` chunking strategy works better, we can move on to our second key question: **Which retrieval method yields the most relevant information?**\n",
    "\n",
    "This experiment will test different points along the search spectrum, from pure keyword matching to pure semantic similarity. We will use the winning `section-based` chunks as the data source for all tests. The methods being compared are:\n",
    "* **Pure BM25**: A traditional keyword search (`alpha=0`).\n",
    "* **Hybrid Search (Keyword-Leaning)**: A mix that gives more weight to keywords (`alpha=0.3`).\n",
    "* **Hybrid Search (Vector-Leaning)**: A mix that gives more weight to semantic meaning (`alpha=0.7`).\n",
    "* **Pure Vector Search**: A pure semantic search (`alpha=1.0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "807e0e59-4036-41c9-988f-27666152f885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPERIMENT 2: SEARCH METHOD SPECTRUM ANALYSIS\n",
      "Testing: BM25 → Hybrid → Vector search (all using section-based chunks)\n",
      "================================================================================\n",
      "Evaluating Section-based + Pure BM25...\n",
      "--------------------------------------------------\n",
      "Q1: When is it correct to use square brackets?...\n",
      "  F1: 0.805\n",
      "Q2: What’s the proper usage of an en dash?...\n",
      "  F1: 0.422\n",
      "Q3: Which spelling is correct: Rhein or Rhin?...\n",
      "  F1: 0.150\n",
      "Q4: Is it better to use the symbol '*' or 'x' for multiplication...\n",
      "  F1: 0.628\n",
      "Q5: Is there a space before or after suspension points?...\n",
      "  F1: 0.192\n",
      "\n",
      "Overall Average F1: 0.439\n",
      "================================================== \n",
      "\n",
      "Evaluating Section-based + Hybrid – keyword-leaning...\n",
      "--------------------------------------------------\n",
      "Q1: When is it correct to use square brackets?...\n",
      "  F1: 0.805\n",
      "Q2: What’s the proper usage of an en dash?...\n",
      "  F1: 0.504\n",
      "Q3: Which spelling is correct: Rhein or Rhin?...\n",
      "  F1: 0.602\n",
      "Q4: Is it better to use the symbol '*' or 'x' for multiplication...\n",
      "  F1: 0.811\n",
      "Q5: Is there a space before or after suspension points?...\n",
      "  F1: 0.522\n",
      "\n",
      "Overall Average F1: 0.649\n",
      "================================================== \n",
      "\n",
      "Evaluating Section-based + Hybrid – vector-leaning...\n",
      "--------------------------------------------------\n",
      "Q1: When is it correct to use square brackets?...\n",
      "  F1: 0.854\n",
      "Q2: What’s the proper usage of an en dash?...\n",
      "  F1: 0.705\n",
      "Q3: Which spelling is correct: Rhein or Rhin?...\n",
      "  F1: 0.539\n",
      "Q4: Is it better to use the symbol '*' or 'x' for multiplication...\n",
      "  F1: 0.868\n",
      "Q5: Is there a space before or after suspension points?...\n",
      "  F1: 0.701\n",
      "\n",
      "Overall Average F1: 0.733\n",
      "================================================== \n",
      "\n",
      "Evaluating Section-based + Pure Vector...\n",
      "--------------------------------------------------\n",
      "Q1: When is it correct to use square brackets?...\n",
      "  F1: 0.781\n",
      "Q2: What’s the proper usage of an en dash?...\n",
      "  F1: 0.705\n",
      "Q3: Which spelling is correct: Rhein or Rhin?...\n",
      "  F1: 0.399\n",
      "Q4: Is it better to use the symbol '*' or 'x' for multiplication...\n",
      "  F1: 0.868\n",
      "Q5: Is there a space before or after suspension points?...\n",
      "  F1: 0.799\n",
      "\n",
      "Overall Average F1: 0.710\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === EXPERIMENT 2: SEARCH METHOD SPECTRUM ANALYSIS ===\n",
    "print('\\nEXPERIMENT 2: SEARCH METHOD SPECTRUM ANALYSIS')\n",
    "print('Testing: BM25 → Hybrid → Vector search (all using section-based chunks)')\n",
    "print('='*80)\n",
    "\n",
    "search_configs = [\n",
    "    (query_semantic_bm25, 'Pure BM25', 'Pure keyword search'),\n",
    "    (lambda q, f, l: query_semantic_hybrid(q, f, l, 0.3), 'Hybrid – keyword-leaning', 'Keyword-focused hybrid'),\n",
    "    (lambda q, f, l: query_semantic_hybrid(q, f, l, 0.7), 'Hybrid – vector-leaning', 'Vector-focused hybrid'),\n",
    "    (query_semantic_vector, 'Pure Vector', 'Pure semantic search')\n",
    "]\n",
    "\n",
    "results_all_methods = []\n",
    "for search_func, method_name, _ in search_configs:\n",
    "    results = evaluate_system(search_func, f'Section-based + {method_name}', ground_truth)\n",
    "    results_all_methods.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bf322de-7e34-4d92-920d-2be4b351feea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SEARCH METHOD ANALYSIS\n",
      "====================================================================================================\n",
      "Method                         Alpha  F1 Score   Improvement\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pure BM25                      0      0.439      Baseline\n",
      "Hybrid – keyword-leaning       0.3    0.649      +47.7%\n",
      "Hybrid – vector-leaning        0.7    0.733      +66.9%\n",
      "Pure Vector                    1.0    0.710      +61.7%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Best performing method: Hybrid – vector-leaning search (α=0.7, F1=0.733)\n"
     ]
    }
   ],
   "source": [
    "# === SEARCH METHOD ANALYSIS AND COMPARISON ===\n",
    "print('\\nSEARCH METHOD ANALYSIS')\n",
    "print('='*100)\n",
    "print(f\"{'Method':<30} {'Alpha':<6} {'F1 Score':<10} {'Improvement'}\")\n",
    "print('-' * 100)\n",
    "\n",
    "alpha_values = [0, 0.3, 0.7, 1.0]\n",
    "search_types = ['Keyword', 'Hybrid – keyword-leaning', 'Hybrid – vector-leaning', 'Semantic']\n",
    "baseline_f1 = results_all_methods[0]['overall_metrics']['avg_content_f1']\n",
    "\n",
    "for i, (_, method_name, _) in enumerate(search_configs):\n",
    "    f1_score = results_all_methods[i]['overall_metrics']['avg_content_f1']\n",
    "    improvement = f'{((f1_score - baseline_f1) / baseline_f1 * 100):+.1f}%' if i > 0 else 'Baseline'\n",
    "    print(f'{method_name:<30} {alpha_values[i]:<6} {f1_score:<10.3f} {improvement}')\n",
    "\n",
    "print('-' * 100)\n",
    "\n",
    "best_idx = max(range(len(results_all_methods)), key=lambda i: results_all_methods[i]['overall_metrics']['avg_content_f1'])\n",
    "best_f1 = results_all_methods[best_idx]['overall_metrics']['avg_content_f1']\n",
    "print(f\"\\nBest performing method: {search_types[best_idx]} search (α={alpha_values[best_idx]}, F1={best_f1:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b884f297-0b1f-4292-b559-47d3f81b3feb",
   "metadata": {},
   "source": [
    "### **7.1. Search Methods: Analysis of Results**\n",
    "\n",
    "The individual F1-scores for each question show a marked trend. The **Pure BM25** keyword search struggles significantly with questions where the phrasing doesn't exactly match the text in the style guide (e.g., Q3: \"Rhein or Rhin\" and Q5: \"suspension points\"), resulting in a low overall F1-score of **0.439**.\n",
    "\n",
    "As soon as we begin to incorporate vector search, performance improves dramatically. Both hybrid methods and the pure vector search show substantial gains over the keyword-only baseline, confirming that semantic understanding is crucial for this task. \n",
    "\n",
    "While all semantic-aware methods performed well, the best results came from the **hybrid search with `alpha=0.7`**. This method, which leans heavily on semantic similarity but still incorporates a small amount of keyword matching, achieved the highest F1-score of **0.733**. This represents a substantial **66.9% improvement** over the pure keyword search baseline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e79e80-7f78-4d8c-952b-b0e314bfff60",
   "metadata": {},
   "source": [
    "## **8. Qualitative Retrieval Analysis**\n",
    "\n",
    "Quantitative metrics like the F1-score are essential for measuring performance, but they don't tell the whole story. To truly understand *why* our optimized system is better, we need to look at the actual text it retrieves.\n",
    "\n",
    "This final analysis provides a **qualitative comparison** for two sample questions. We will compare the initial, un-optimized baseline (`Fixed-size + Vector`) against our final, fully optimized system (`Section-based + Hybrid – vector-leaning`). Questions 3 and 4 were chosen as they provide excellent examples of different retrieval challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef01bf80-4de2-456f-ab53-dd4992245b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAMPLE RETRIEVAL ANALYSIS: BASELINE vs OPTIMIZED\n",
      "================================================================================\n",
      "\n",
      "Question 3: Which spelling is correct: Rhein or Rhin?\n",
      "Baseline F1: 0.322 → Optimized F1: 0.539\n",
      "===========================================================================\n",
      "BASELINE (Fixed-size + Vector):\n",
      "  1. Relevance: 0.664 | Text: useful to add ‘region’ or ‘area’ in such cases), Lüneburger Heide \n",
      "♦ Officially designated development areas. Designated development areas are \n",
      "mostly derived from names of administrative units or from traditional \n",
      "geographical names, often with a defining adjective. Follow the appropriate \n",
      "rule above, e.g.: \n",
      "Lower Bavaria; the Charentes development area \n",
      "The name of the cross-border region Euregio is written with an initial capital \n",
      "only. \n",
      "5.22. \n",
      "Rivers. Use the forms Meuse (Maas only if the context is solely Flanders or the \n",
      "Netherlands) and Moselle (Mosel only if the context is solely Germany). Write \n",
      "Rhine for Rhein, Rhin, and Rijn, and Rhineland for Rheinland. Also: Oder for \n",
      "Odra (Poli...\n",
      "  2. Relevance: 0.613 | Text: Нн \n",
      "n \n",
      "Њњ \n",
      "- \n",
      "- \n",
      "nj \n",
      "- \n",
      "- \n",
      "nj \n",
      "Оо \n",
      "o \n",
      "Пп \n",
      "p \n",
      " \n",
      "1  \n",
      "When preceded by з to avoid confusion with the digraph ‘zh’ that represents ж: for example, \n",
      "Згорани becomes Zghorany. \n",
      "2  \n",
      "The transliteration ‘dj’ is sometimes seen but considered incorrect, so for Ђоковић write Ðoković, \n",
      "not Djoković. \n",
      "3  \n",
      "Initially and after a vowel, apostrophe, soft sign or ў. \n",
      "4  \n",
      "Initially and after vowel. \n",
      "5  \n",
      "Initially. \n",
      "6  \n",
      "The combination ‘ий’ should be transliterated as ‘i’ in Russian, but as ‘yi’ in Ukrainian. \n",
      " \n",
      "  \n",
      "  \n",
      "Letter \n",
      "BE \n",
      "BG \n",
      "MK \n",
      "RU \n",
      "UK \n",
      "SR* \n",
      "Рр \n",
      "r \n",
      "Сс \n",
      "s \n",
      "Тт \n",
      "t \n",
      "Ћћ \n",
      "- \n",
      "-...\n",
      "  3. Relevance: 0.598 | Text: river called the Labe in Czech is known as the Elbe in English. \n",
      "If included at all, the word ‘river’ normally precedes the proper name (the \n",
      "River Thames), unless it is regarded as an integral part of the name (the Yellow \n",
      "River). In either case, it takes a capital letter. \n",
      "5.23. \n",
      "Seas. Anglicise seas (e.g. the Adriatic, the North Sea, the Baltic); Greenland \n",
      "waters implies official sea limits; use ‘waters off Greenland’ if something else \n",
      "is meant. \n",
      "5.24. \n",
      "Lakes. Use the English names Lake Constance (for Bodensee), Lake Geneva \n",
      "(for Lac Léman), Lake Maggiore (for Lago Maggiore) and Lake Balaton (for \n",
      "Balaton). \n",
      "5.25. \n",
      "Strait/straits. The singular is the form commonly used in official names...\n",
      "  4. Relevance: 0.598 | Text: English or French (e.g. DE: Scheich Jamani = EN: Sheikh Yamani). Note \n",
      "spellings of Maghreb and Mashreq. \n",
      "The article Al and variants should be capitalised at the beginning of names but \n",
      "not internally: Dhu al Faqar, Abd ar Rahman. Do not use hyphens to connect \n",
      "parts of a name. \n",
      "9.7. \n",
      "Chinese. The pinyin romanisation system introduced by the People’s Republic \n",
      "in the 1950s has now become the internationally accepted standard. Important \n",
      "new spellings to note are: \n",
      "Beijing \n",
      "(Peking) \n",
      "Guangzhou \n",
      "(Canton) \n",
      "Nanjing \n",
      "(Nanking) \n",
      "Xinjiang  \n",
      "(Sinkiang) \n",
      "The spelling of Shanghai remains the same. \n",
      "Add the old form in parentheses if you think it necessary. \n",
      "Geographical names and other proper nouns w...\n",
      "  5. Relevance: 0.596 | Text: example: Strait of Dover or Strait of Gibraltar. \n",
      "5.26. \n",
      "Other bodies of water. Write IJsselmeer (not Ij- or Y-), Wattenmeer, Kattegat \n",
      "(Danish), Kattegatt (Swedish), Great/Little Belt. \n",
      "5.27. \n",
      "Islands. Islands are often administrative units in their own right, so leave in \n",
      "original spelling, except Corsica, Sicily, Sardinia, the Canary Islands, the \n",
      "Azores and Greek islands with accepted English spellings, such as Crete, \n",
      "Corfu, Lesbos. \n",
      " \n",
      "  \n",
      "  \n",
      "Use Fyn rather than Fünen in English texts and use West Friesian Islands for \n",
      "Waddeneilanden. \n",
      "5.28. \n",
      "Mountains. Anglicise the Alps, Apennines (one p), Dolomites, Pindus \n",
      "Mountains, and Pyrenees (no accents). \n",
      "Do not anglicise Massif Central (except...\n",
      "\n",
      "OPTIMIZED (Section-based + Hybrid – vector-leaning):\n",
      "  1. Context: 5.   Names and titles > Country/territory  City/town | Relevance: 0.857\n",
      "     Text: 5.22.   Rivers . Use the forms  Meuse  ( Maas  only if the context is solely Flanders or the  Netherlands) and  Moselle  ( Mosel  only if the context is solely Germany). Write  Rhine  for  Rhein ,  Rhin , and  Rijn , and  Rhineland  for  Rheinland.  Also:  Oder  for   Odra (Polish and Czech);   Tiber  for  Tevere ;  Tagus  for  Tajo / Tejo . Note that the  river called the  Labe  in Czech is known as the  Elbe  in English.\n",
      "If included at all, the word ‘river’ normally precedes the proper name ( the  River Thames ), unless it is regarded as an integral part of the name ( the Yellow  River ). In either case, it takes a capital letter....\n",
      "  2. Context: 3.   Spelling > Conventions | Relevance: 0.583\n",
      "     Text: 3.2.   -is-/-iz- spelling . Use the  -is-  spelling. Both spellings are correct, but this rule is  to be followed for the sake of consistency in EU texts....\n",
      "  3. Context: 16.   Science guide > N/A | Relevance: 0.474\n",
      "     Text: 16.8.   Sulphur/sulfur.  Note that the spelling  sulfur  is preferred by the  International  Union of Pure and Applied Chemistry  (IUPAC), but the Harmonised System  and Combined Nomenclature (customs tariff nomenclatures) retain the  sulph-   forms. The correct spelling will therefore depend on the context....\n",
      "  4. Context: 3.   Spelling > Conventions | Relevance: 0.367\n",
      "     Text: 3.6.   Digraphs.  Keep the digraph in  aetiology, caesium, oenology, oestrogen,  etc.  ( etiology  etc. are US usage), but note that a number of such words  (e.g.  medieval  and  fetus ) are now normally spelled without the digraph in  Irish/British English.  Foetus  is still common in Ireland and the United  Kingdom in non-technical use....\n",
      "  5. Context: 3.   Spelling > N/A | Relevance: 0.356\n",
      "     Text: 3.   Spelling...\n",
      "===========================================================================\n",
      "\n",
      "Question 4: Is it better to use the symbol '*' or 'x' for multiplication?\n",
      "Baseline F1: 0.558 → Optimized F1: 0.868\n",
      "===========================================================================\n",
      "BASELINE (Fixed-size + Vector):\n",
      "  1. Relevance: 0.758 | Text: accordingly and be as clear and grammatical as any other type of text. \n",
      "7.13. \n",
      "Mathematical symbols. Always use the correct character for mathematical \n",
      "symbols. For example, do not use an en dash as a minus sign1 or a letter ‘x’ as \n",
      "a multiplication sign2, as these can make equations inaccessible to people using \n",
      "screen readers. \n",
      " \n",
      "1  \n",
      "For the minus sign (−), use Alt + 8722 in Windows and U + 2212 in Unicode. \n",
      "2  \n",
      "For the multiplication sign (×), use Alt + 0215 in Windows and U + 00D7 in Unicode.   \n",
      " \n",
      "There should be a hard space1 between the symbol and the number, thus: \n",
      "10 ÷ 5 = 2, and \n",
      "10 − 11 = −1. \n",
      "The minus sign should normally be followed by a hard space1 (see Section 6.4 \n",
      "of the Inte...\n",
      "  2. Relevance: 0.719 | Text: of the Interinstitutional Style Guide), but when expressing a negative value in a \n",
      "mathematical equation, it should be closed up to the following figure to avoid \n",
      "confusion (as in the example above). \n",
      "7.14. \n",
      "Foreign-language conventions. Remember that languages may have different \n",
      "conventions as regards their use of mathematical symbols, especially those for \n",
      "multiplication, division, and subtraction. \n",
      "Many mathematical symbols also have several different meanings according to \n",
      "the context. \n",
      "7.15. \n",
      "Multiplication sign. Change a point or a raised dot used as a multiplication sign \n",
      "to × or *, e.g. 2.6 · 1018 becomes 2.6 × 1018 or 2.6 * 1018. A point used in an \n",
      "algebraic expression can be omit...\n",
      "  3. Relevance: 0.665 | Text: Note, however, that a raised dot can have other meanings too (see Wikipedia). \n",
      "7.16. \n",
      "Division sign. In the English-speaking world, the commonest symbols for \n",
      "division are ÷ , / , and ∕ (obelus2, slash, and solidus or division slash3). In other \n",
      "countries : (colon) is very widely used to denote division. \n",
      "Note that in some countries (Norway, for one) ÷ can denote subtraction (!), and \n",
      "in Italy it can also denote a range (e.g. 40% ÷ 50% means 40 to 50 per cent). \n",
      "7.17. \n",
      "Ranges. Use a closed-up en dash or hyphen to signify a range (e.g. 10–12%). \n",
      "Note the remark concerning Italian usage in 7.16. See also 2.17 and 2.18 on \n",
      "dashes. \n",
      "7.18. \n",
      "Technical tolerances. Do not use ± (ASCII 241) to mean ‘...\n",
      "  4. Relevance: 0.634 | Text: by an amount, use the ISO code (compulsory in legal acts). It is followed by a \n",
      "hard space1 and the amount in figures: \n",
      "The amount required is EUR 500 \n",
      "A sum of USD 300 was spent \n",
      "The main ISO codes are set out in Annex A7 to the Interinstitutional Style \n",
      "Guide. An exhaustive list of codes can be found in ISO 4217. \n",
      "In graphics and infographics, popular works, promotional publications, press \n",
      "releases and audiovisual products and when writing for the web, the currency \n",
      "symbol (€, $, etc.) should be used. It is closed up to the figure: \n",
      "The amount required is €500 \n",
      "A sum of $300 was spent \n",
      "8.2. \n",
      "Negative currency values. Where a minus sign is used to express a negative \n",
      "amount, it precedes th...\n",
      "  5. Relevance: 0.632 | Text: stremmata, for land measurement. Aircraft altitudes are often expressed in feet \n",
      "(ft). Do not convert quantities, although an explanatory footnote may be \n",
      "inserted if appropriate. \n",
      "7.26. \n",
      "Degree sign. The degree sign in temperatures should be preceded by a hard \n",
      "space1, e.g. 25 °C. In other cases, the degree sign is closed up with the \n",
      "preceding number (e.g. 65° NE). See also Section 6.4 of the Interinstitutional \n",
      "Style Guide. \n",
      "7.27. \n",
      "Ohm. The ohm symbol is capital omega (Ω). All other SI symbols for units of \n",
      "measurement are formed from unaccented Latin characters. \n",
      " \n",
      " \n",
      "7.28. \n",
      "In computing, K (kilo), M (mega) and G (giga) often stand for binary thousands \n",
      "(1 024=210), millions (1 048 576=22...\n",
      "\n",
      "OPTIMIZED (Section-based + Hybrid – vector-leaning):\n",
      "  1. Context: 7.   Abbreviations, symbols and units of measurement > Mathematical symbols | Relevance: 1.000\n",
      "     Text: 7.13.   Mathematical symbols . Always use the correct character for mathematical  symbols. For example, do not use an en dash as a minus sign 1  or a letter ‘x’ as  a multiplication sign 2 , as these can make equations inaccessible to people using  screen readers.\n",
      "1    For the minus sign (−), use Alt + 8722 in Windows and U + 2212 in Unicode.\n",
      "2    For the multiplication sign (×), use Alt + 0215 in Windows and U + 00D7 in Unicode.\n",
      "  There should be a hard space 1  between the symbol and the number, thus:\n",
      "10 ÷ 5 = 2, and\n",
      "10 − 11 = −1.\n",
      "The minus sign should normally be followed by a hard space 1  (see  Section 6.4   of the  Interinstitutional Style Guide ), but when expressing a negative value ...\n",
      "  2. Context: 7.   Abbreviations, symbols and units of measurement > Mathematical symbols | Relevance: 0.826\n",
      "     Text: 7.15.   Multiplication sign.  Change a point or a raised dot used as a multiplication sign  to × or *, e.g.  2.6 · 10 18   becomes  2.6 × 10 18  or  2.6 * 10 18 . A point used in an  algebraic expression can be omitted, e.g.  2·A = 2·π·r 2  can be written  2A = 2πr 2 .\n",
      "Note, however, that a raised dot can have other meanings too (see  Wikipedia )....\n",
      "  3. Context: 7.   Abbreviations, symbols and units of measurement > Mathematical symbols | Relevance: 0.713\n",
      "     Text: 7.14.   Foreign-language conventions.  Remember that languages may have different  conventions as regards their use of mathematical symbols, especially those for  multiplication, division, and subtraction.\n",
      "Many mathematical symbols also have several different meanings according to  the context....\n",
      "  4. Context: 7.   Abbreviations, symbols and units of measurement > Mathematical symbols | Relevance: 0.597\n",
      "     Text: Mathematical symbols...\n",
      "  5. Context: 7.   Abbreviations, symbols and units of measurement > Scientific symbols and units of measurement | Relevance: 0.423\n",
      "     Text: Scientific symbols and units of measurement...\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "# === SAMPLE RETRIEVED CHUNKS ANALYSIS ===\n",
    "print('\\nSAMPLE RETRIEVAL ANALYSIS: BASELINE vs OPTIMIZED')\n",
    "print('='*80)\n",
    "\n",
    "def display_retrieval_comparison(question_id, baseline_results, optimized_results, max_chars=700):\n",
    "    \"\"\"Compares retrieved chunks between baseline and optimized methods.\"\"\"\n",
    "    baseline_q = baseline_results['question_scores'][question_id]\n",
    "    optimized_q = optimized_results['question_scores'][question_id]\n",
    "    \n",
    "    print(f\"\\nQuestion {question_id}: {baseline_q['question']}\")\n",
    "    print(f\"Baseline F1: {baseline_q['metrics']['content_f1']:.3f} → Optimized F1: {optimized_q['metrics']['content_f1']:.3f}\")\n",
    "    print('=' * 75)\n",
    "    \n",
    "    print('BASELINE (Fixed-size + Vector):')\n",
    "    for i, chunk in enumerate(baseline_q['retrieved_chunks'], 1):\n",
    "        print(f\"  {i}. Relevance: {chunk['relevance_score']:.3f} | Text: {chunk['text'][:max_chars]}...\")\n",
    "    \n",
    "    print(f\"\\nOPTIMIZED (Section-based + {search_types[best_idx]}):\")\n",
    "    for i, chunk in enumerate(optimized_q['retrieved_chunks'], 1):\n",
    "        context_str = f\"{chunk.get('chapter', 'N/A')} > {chunk.get('section', 'N/A')}\"\n",
    "        print(f\"  {i}. Context: {context_str} | Relevance: {chunk['relevance_score']:.3f}\")\n",
    "        print(f\"     Text: {chunk['text'][:max_chars]}...\")\n",
    "    print('=' * 75)\n",
    "\n",
    "display_retrieval_comparison('3', results_fixed, results_all_methods[best_idx])\n",
    "display_retrieval_comparison('4', results_fixed, results_all_methods[best_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636b6e5e-b3cc-4a81-91ef-c8499f5bfb25",
   "metadata": {},
   "source": [
    "### **8.1. Qualitative Analysis: Results**\n",
    "\n",
    "#### **Question 3 (\"Which spelling is correct: Rhein or Rhin?\"):**\n",
    "* **The baseline problem is the chunk, not the retrieval:** The baseline system correctly retrieves the relevant passage about river names as its top result. The issue is the `fixed-size` chunking strategy. The correct answer is buried in a chunk diluted with irrelevant text about development areas, which harms the precision score and brings down the overall F1-score.\n",
    "* **The optimized system is precise:** The optimized system excels because its top `section-based` chunk for this topic contains *only* the relevant text on \"Rivers.\" This clean, focused chunk is easily found by the hybrid search, resulting in a much higher F1-score (0.539 vs 0.322). This demonstrates that a precise chunking strategy is just as critical as the search algorithm.\n",
    "\n",
    "#### **Question 4 (\"Is it better to use the symbol '*' or 'x' for multiplication?\"):**\n",
    "* **The baseline is noisy and fragmented:** The baseline system again finds the correct information, but it's presented along with unrelated rules. This noise explains why the F1-score is only 0.558.\n",
    "* **The optimized system is focused:** The optimized system retrieves chunks that are all from the correct chapter (\"Abbreviations, symbols and units of measurement\") and section (\"Mathematical symbols\"). The top result is the exact passage answering the question, earning a perfect relevance score of 1.000. This precision leads to a far superior F1-score of 0.868.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a235d-e769-4b85-a211-cf5441a3ccdf",
   "metadata": {},
   "source": [
    "## **9. Final Summary and Key Findings**\n",
    "\n",
    "Now that the qualitative analysis is complete, the next cell provides a high-level summary of the entire evaluation. It quantifies the total improvement from our initial baseline to our final optimized system, summarizing the key findings of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6c1f227-b1f2-4363-8f3a-83ed557391d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL COMPARISON: BASELINE vs OPTIMIZED\n",
      "=====================================================================================\n",
      "System                                   F1 Score  Improvement\n",
      "-------------------------------------------------------------------------------------\n",
      "Fixed-size + Vector                      0.534     Baseline\n",
      "Section-based + Hybrid – vector-leaning  0.733     +37.3%\n",
      "-------------------------------------------------------------------------------------\n",
      "\n",
      "EXPERIMENTS COMPLETE!\n",
      "=====================================================================================\n",
      "Key Findings:\n",
      "1. Chunking Strategy: Section-based chunks improved F1 score over fixed-size\n",
      "2. Search Method: Hybrid – vector-leaning (α=0.7) outperformed pure approaches\n",
      "3. Overall Optimization: +37.3% improvement from baseline to optimized system\n",
      "4. Next Step: Implement RAG with prompt engineering in notebook 04\n"
     ]
    }
   ],
   "source": [
    "# === FINAL COMPARISON AND KEY FINDINGS ===\n",
    "print('\\nFINAL COMPARISON: BASELINE vs OPTIMIZED')\n",
    "print('='*85)\n",
    "\n",
    "baseline_metrics = results_fixed['overall_metrics']\n",
    "optimized_metrics = results_all_methods[best_idx]['overall_metrics']\n",
    "improvement = f\"{((optimized_metrics['avg_content_f1'] - baseline_metrics['avg_content_f1']) / baseline_metrics['avg_content_f1'] * 100):+.1f}%\"\n",
    "optimized_name = f'Section-based + {search_types[best_idx]}'\n",
    "\n",
    "print(f\"{'System':<40} {'F1 Score':<9} {'Improvement'}\")\n",
    "print('-' * 85)\n",
    "print(f\"{'Fixed-size + Vector':<40} {baseline_metrics['avg_content_f1']:<9.3f} {'Baseline'}\")\n",
    "print(f'{optimized_name:<40} {optimized_metrics[\"avg_content_f1\"]:<9.3f} {improvement}')\n",
    "print('-' * 85)\n",
    "\n",
    "print('\\nEXPERIMENTS COMPLETE!')\n",
    "print('='*85)\n",
    "print('Key Findings:')\n",
    "print('1. Chunking Strategy: Section-based chunks improved F1 score over fixed-size')\n",
    "print(f'2. Search Method: {search_types[best_idx]} (α={alpha_values[best_idx]}) outperformed pure approaches')\n",
    "print(f'3. Overall Optimization: {improvement} improvement from baseline to optimized system')\n",
    "print('4. Next Step: Implement RAG with prompt engineering in notebook 04')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b19d44-0f55-422e-b52d-f17cfa22b7c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "By moving from a simple `Fixed-size + Vector` approach to a more sophisticated `Section-based + Hybrid` system, we achieved a significant performance boost. The final F1-score of **0.733** represents a **+37.3% improvement** over our initial baseline. \n",
    "\n",
    "Here are the two main findings of our experiments:\n",
    "1.  **Chunking Strategy Matters:** `Section-based` chunks, which preserve the document's logical structure, are considerably more effective than simple `fixed-size` chunks.\n",
    "2.  **Hybrid Search is Superior:** A `Hybrid – vector-leaning` search (`α=0.7`) outperforms both pure keyword and pure vector approaches, blending the benefits of both.\n",
    "\n",
    "As a final step, the full results of this notebook are exported to a timestamped JSON file for record-keeping before the connection to Weaviate is closed. With these data-driven decisions made, we are now ready to build the final RAG pipeline in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5b84060-7166-49c6-beba-a936c18ff81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results exported to: ../results/retrieval_evaluation_results.json\n",
      "Weaviate client connection closed.\n"
     ]
    }
   ],
   "source": [
    "# === EXPORT RESULTS AND CLOSE CONNECTION ===\n",
    "def export_retrieval_analysis():\n",
    "    \"\"\"Exports complete experimental results to a JSON file.\"\"\"\n",
    "    analysis = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'notebook': '03_chunking_and_retrieval_evaluation',\n",
    "        'experiments': {\n",
    "            'chunking_comparison': {\n",
    "                'fixed_size': results_fixed,\n",
    "                'section_based': results_section\n",
    "            },\n",
    "            'search_method_comparison': {\n",
    "                'methods': results_all_methods,\n",
    "                'best_method_idx': best_idx,\n",
    "                'best_method_name': search_types[best_idx]\n",
    "            }\n",
    "        },\n",
    "        'ground_truth_summary': {\n",
    "            'total_questions': len(ground_truth),\n",
    "            'questions': {q_id: data['question'] for q_id, data in ground_truth.items()}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    os.makedirs('../results', exist_ok=True)\n",
    "    with open('../results/retrieval_evaluation_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print('Results exported to: ../results/retrieval_evaluation_results.json')\n",
    "\n",
    "export_retrieval_analysis()\n",
    "\n",
    "client.close()\n",
    "print('Weaviate client connection closed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
