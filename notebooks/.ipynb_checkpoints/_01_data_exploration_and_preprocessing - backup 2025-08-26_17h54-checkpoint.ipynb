{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57896f-0204-4572-82c4-c11cfea6a651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a65fbd5a-108c-469d-bf98-7d69c0451098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF with fixed-size chunking...\n",
      "Generated 152 chunks\n",
      "Average tokens per chunk: 509.6\n",
      "Saved chunks to ../data/processed/fixed_size_chunks.json\n",
      "\n",
      "=== SAMPLE CHUNKS ===\n",
      "\n",
      "Chunk 1 (Page 4):\n",
      "Tokens: 512\n",
      "Method: fixed_size\n",
      "Text preview: A handbook for authors and translators in the European Commission \n",
      "Eighth edition: January 2016 \n",
      "Last updated: February 2025 \n",
      "See also the Country Compendium, a companion to the English Style Guide. \n",
      "...\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 2 (Page 5):\n",
      "Tokens: 512\n",
      "Method: fixed_size\n",
      "Text preview: and percentages ............................................................................... 39 \n",
      "Ranges ................................................................................................\n",
      "--------------------------------------------------\n",
      "\n",
      "Chunk 3 (Page 7):\n",
      "Tokens: 512\n",
      "Method: fixed_size\n",
      "Text preview: ...... 88 \n",
      "Referring to subdivisions of acts ..................................................................... 90 \n",
      "20. \n",
      "The EU institutions ...........................................................\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pymupdf as fitz\n",
    "import json\n",
    "import tiktoken\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "def create_fixed_size_chunks(pdf_path: str, max_tokens: int = 512, overlap_tokens: int = 100):\n",
    "    \"\"\"\n",
    "    Create fixed-size chunks with token overlap for baseline evaluation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {e}. Falling back to character-based splitting.\")\n",
    "        tokenizer = None\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    fixed_chunks = []\n",
    "    fixed_chunk_id = 0\n",
    "    current_chunk_text = \"\"\n",
    "\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "\n",
    "        for block in blocks:\n",
    "            text = block[4]  # The fifth element is the text\n",
    "            \n",
    "            if not text or len(text.strip()) < 50:\n",
    "                continue\n",
    "\n",
    "            current_chunk_text += text\n",
    "\n",
    "            if tokenizer:\n",
    "                tokens = tokenizer.encode(current_chunk_text)\n",
    "                \n",
    "                while len(tokens) >= max_tokens:\n",
    "                    split_tokens = tokens[:max_tokens]\n",
    "                    split_text = tokenizer.decode(split_tokens)\n",
    "\n",
    "                    fixed_chunks.append({\n",
    "                        'chunk_id': f'fixed_{fixed_chunk_id:04d}',\n",
    "                        'text': split_text.strip(),\n",
    "                        'page_number': page_num + 1,\n",
    "                        'token_count': len(split_tokens),\n",
    "                        'method': 'fixed_size'\n",
    "                    })\n",
    "                    fixed_chunk_id += 1\n",
    "\n",
    "                    # Prepare overlap for next chunk\n",
    "                    overlap_tokens_list = split_tokens[-overlap_tokens:]\n",
    "                    overlap_text = tokenizer.decode(overlap_tokens_list)\n",
    "                    current_chunk_text = overlap_text + current_chunk_text[len(split_text):]\n",
    "                    tokens = tokenizer.encode(current_chunk_text)\n",
    "\n",
    "            else:\n",
    "                # Fallback to character-based splitting\n",
    "                char_limit = max_tokens * 4\n",
    "                while len(current_chunk_text) >= char_limit:\n",
    "                    split_text = current_chunk_text[:char_limit]\n",
    "                    \n",
    "                    fixed_chunks.append({\n",
    "                        'chunk_id': f'fixed_{fixed_chunk_id:04d}',\n",
    "                        'text': split_text.strip(),\n",
    "                        'page_number': page_num + 1,\n",
    "                        'token_count': None,\n",
    "                        'method': 'fixed_size'\n",
    "                    })\n",
    "                    fixed_chunk_id += 1\n",
    "                    \n",
    "                    overlap_chars = overlap_tokens * 4\n",
    "                    current_chunk_text = current_chunk_text[char_limit - overlap_chars:]\n",
    "\n",
    "    # Add final chunk if remaining text\n",
    "    if current_chunk_text.strip():\n",
    "        if tokenizer:\n",
    "            tokens = tokenizer.encode(current_chunk_text)\n",
    "            token_count = len(tokens)\n",
    "        else:\n",
    "            token_count = None\n",
    "            \n",
    "        fixed_chunks.append({\n",
    "            'chunk_id': f'fixed_{fixed_chunk_id:04d}',\n",
    "            'text': current_chunk_text.strip(),\n",
    "            'page_number': doc.page_count,\n",
    "            'token_count': token_count,\n",
    "            'method': 'fixed_size'\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "    return fixed_chunks\n",
    "\n",
    "# Test and save results\n",
    "pdf_path = \"../data/raw/English_Style_Guide-European_Commission.pdf\"\n",
    "fixed_size_chunks = create_fixed_size_chunks(pdf_path)\n",
    "\n",
    "print(\"Processing PDF with fixed-size chunking...\")\n",
    "print(f\"Generated {len(fixed_size_chunks)} chunks\")\n",
    "\n",
    "if fixed_size_chunks and fixed_size_chunks[0]['token_count'] is not None:\n",
    "    avg_tokens = sum(c['token_count'] for c in fixed_size_chunks if c['token_count']) / len([c for c in fixed_size_chunks if c['token_count']])\n",
    "    print(f\"Average tokens per chunk: {avg_tokens:.1f}\")\n",
    "\n",
    "# Save results\n",
    "fixed_output_file = '../data/processed/fixed_size_chunks.json'\n",
    "with open(fixed_output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(fixed_size_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved chunks to {fixed_output_file}\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\n=== SAMPLE CHUNKS ===\")\n",
    "for i, chunk in enumerate(fixed_size_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (Page {chunk['page_number']}):\")\n",
    "    print(f\"Tokens: {chunk['token_count']}\")\n",
    "    print(f\"Method: {chunk['method']}\")\n",
    "    print(f\"Text preview: {chunk['text'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0704c0-dae6-4055-b8fd-0239f397fe8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a61257-bcd2-418d-ba4a-8b7eaac1293f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7bf80d-4414-4b7c-86db-a10c0675414f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4c4de4c-87b6-4d0e-9746-eee0bad65f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"level\": 1,\n",
      "    \"title\": \"English Style Guide\",\n",
      "    \"page\": 1\n",
      "  },\n",
      "  {\n",
      "    \"level\": 1,\n",
      "    \"title\": \"Introduction\",\n",
      "    \"page\": 7\n",
      "  },\n",
      "  {\n",
      "    \"level\": 1,\n",
      "    \"title\": \"Part I  Writing English\",\n",
      "    \"page\": 9\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"1. General\",\n",
      "    \"page\": 10\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"2. Punctuation\",\n",
      "    \"page\": 10\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Full stop\",\n",
      "    \"page\": 10\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Colon\",\n",
      "    \"page\": 11\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Semicolon\",\n",
      "    \"page\": 11\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Comma\",\n",
      "    \"page\": 12\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Dashes\",\n",
      "    \"page\": 15\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Brackets\",\n",
      "    \"page\": 16\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Question mark\",\n",
      "    \"page\": 16\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Exclamation mark\",\n",
      "    \"page\": 17\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Quotation marks\",\n",
      "    \"page\": 17\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Forward slash\",\n",
      "    \"page\": 19\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Apostrophe\",\n",
      "    \"page\": 19\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"3. Spelling\",\n",
      "    \"page\": 21\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Conventions\",\n",
      "    \"page\": 21\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Interference effects\",\n",
      "    \"page\": 24\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Compound words and hyphens\",\n",
      "    \"page\": 25\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"4. Capitalisation\",\n",
      "    \"page\": 28\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"5. Names and titles\",\n",
      "    \"page\": 35\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Personal names and titles\",\n",
      "    \"page\": 35\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Names of bodies\",\n",
      "    \"page\": 37\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Names of ships, aircraft and other vehicles\",\n",
      "    \"page\": 38\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Geographical names\",\n",
      "    \"page\": 39\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"6. Numbers\",\n",
      "    \"page\": 42\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Writing out numbers\",\n",
      "    \"page\": 42\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Serial numbers\",\n",
      "    \"page\": 44\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Fractions and percentages\",\n",
      "    \"page\": 45\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Ranges\",\n",
      "    \"page\": 46\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Dates and times\",\n",
      "    \"page\": 47\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"7. Abbreviations, symbols and units of measurement\",\n",
      "    \"page\": 49\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Abbreviations\",\n",
      "    \"page\": 49\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Mathematical symbols\",\n",
      "    \"page\": 53\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Scientific symbols and units of measurement\",\n",
      "    \"page\": 54\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"8. Currencies\",\n",
      "    \"page\": 56\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"9. Foreign imports\",\n",
      "    \"page\": 57\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Foreign words and phrases in English text\",\n",
      "    \"page\": 57\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Romanisation systems\",\n",
      "    \"page\": 58\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"10. Parts of speech\",\n",
      "    \"page\": 59\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Adjectives and adverbs\",\n",
      "    \"page\": 59\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Nouns: singular or plural\",\n",
      "    \"page\": 61\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Nouns: people or persons\",\n",
      "    \"page\": 63\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Verbs: present perfect/simple past\",\n",
      "    \"page\": 63\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Verbs: tenses in minutes\",\n",
      "    \"page\": 64\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Verbs: usage in legislation, contracts and the like\",\n",
      "    \"page\": 64\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Verbs: split infinitive\",\n",
      "    \"page\": 67\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Verbs: the -ing form and the possessive\",\n",
      "    \"page\": 67\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Conjunctions\",\n",
      "    \"page\": 68\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"11. Lists\",\n",
      "    \"page\": 68\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"12. Legal language\",\n",
      "    \"page\": 70\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"13. Footnotes, citations and references\",\n",
      "    \"page\": 71\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"14. Correspondence\",\n",
      "    \"page\": 73\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"15. Inclusive language\",\n",
      "    \"page\": 74\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"16. Science guide\",\n",
      "    \"page\": 77\n",
      "  },\n",
      "  {\n",
      "    \"level\": 1,\n",
      "    \"title\": \"Part II  About the European Union\",\n",
      "    \"page\": 82\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"17. The European Union\",\n",
      "    \"page\": 83\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"18. Primary legislation\",\n",
      "    \"page\": 84\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"The treaties – an overview\",\n",
      "    \"page\": 84\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"The treaties in detail\",\n",
      "    \"page\": 85\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Treaty citations\",\n",
      "    \"page\": 88\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"19. Secondary legislation\",\n",
      "    \"page\": 89\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Legislative procedures\",\n",
      "    \"page\": 89\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Titles and numbering\",\n",
      "    \"page\": 90\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Structure of acts\",\n",
      "    \"page\": 94\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Referring to subdivisions of acts\",\n",
      "    \"page\": 96\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"20. The EU institutions\",\n",
      "    \"page\": 97\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Commission\",\n",
      "    \"page\": 97\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Council\",\n",
      "    \"page\": 98\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"European Council\",\n",
      "    \"page\": 99\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"European Parliament\",\n",
      "    \"page\": 99\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Court of Justice of the European Union\",\n",
      "    \"page\": 100\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"European Court of Auditors\",\n",
      "    \"page\": 103\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"European Economic and Social Committee\",\n",
      "    \"page\": 103\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Committee of the Regions\",\n",
      "    \"page\": 104\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"European Central Bank\",\n",
      "    \"page\": 104\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Other financial institutions\",\n",
      "    \"page\": 104\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Interinstitutional bodies\",\n",
      "    \"page\": 105\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Agencies\",\n",
      "    \"page\": 105\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"21. References to official publications\",\n",
      "    \"page\": 105\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"The Official Journal\",\n",
      "    \"page\": 105\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"General Report and Bulletin\",\n",
      "    \"page\": 106\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"22. EU finances\",\n",
      "    \"page\": 107\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Budget\",\n",
      "    \"page\": 107\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Programmes and funds financed from the EU budget and NextGenerationEU\",\n",
      "    \"page\": 109\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Other funds\",\n",
      "    \"page\": 110\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"23. Member States\",\n",
      "    \"page\": 111\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"Permanent Representations/Representatives\",\n",
      "    \"page\": 111\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"National parliaments\",\n",
      "    \"page\": 112\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"National judicial bodies\",\n",
      "    \"page\": 112\n",
      "  },\n",
      "  {\n",
      "    \"level\": 3,\n",
      "    \"title\": \"National legislation\",\n",
      "    \"page\": 112\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"24. Official languages\",\n",
      "    \"page\": 115\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"25. External relations\",\n",
      "    \"page\": 116\n",
      "  },\n",
      "  {\n",
      "    \"level\": 1,\n",
      "    \"title\": \"Annexes\",\n",
      "    \"page\": 119\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"Annex 1 International organisations whose names do not follow our standard spelling rules\",\n",
      "    \"page\": 120\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"Annex 2 Transliteration table for Greek\",\n",
      "    \"page\": 122\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"Annex 3 Conversion table for Greek serial numbering\",\n",
      "    \"page\": 125\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"Annex 4 Transliteration table for Cyrillic\",\n",
      "    \"page\": 127\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"Annex 5 Conversion table for Bulgarian serial numbering\",\n",
      "    \"page\": 130\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"Annex 6 Conversion table for numbering of inserted articles\",\n",
      "    \"page\": 131\n",
      "  },\n",
      "  {\n",
      "    \"level\": 2,\n",
      "    \"title\": \"Annex 7 Forms of address\",\n",
      "    \"page\": 132\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pymupdf as fitz\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def get_toc_from_metadata(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parses the document's TOC using PyMuPDF's get_toc() method, which extracts\n",
    "    the official table of contents from the PDF's metadata. This is the most\n",
    "    reliable method for getting the hierarchical structure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        toc_entries = doc.get_toc(simple=False) # Get detailed TOC with levels\n",
    "        doc.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening or getting TOC from PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "    return toc_entries\n",
    "\n",
    "# The path to the PDF file.\n",
    "pdf_path = \"../data/raw/English_Style_Guide-European_Commission.pdf\"\n",
    "\n",
    "# Get the hierarchical TOC\n",
    "toc_hierarchy = get_toc_from_metadata(pdf_path)\n",
    "\n",
    "# Print the result in a nicely formatted JSON string\n",
    "# The `toc_hierarchy` is a list of lists, where each inner list is\n",
    "# [level, title, page_number, xref], so we'll reformat it for clarity.\n",
    "formatted_toc = []\n",
    "for level, title, page, xref in toc_hierarchy:\n",
    "    formatted_toc.append({\n",
    "        \"level\": level,\n",
    "        \"title\": title,\n",
    "        \"page\": page\n",
    "    })\n",
    "\n",
    "print(json.dumps(formatted_toc, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31456f-05cf-4c93-ab2b-84c07c1d558d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c391b36-efc2-405c-9f73-1ffa9084187d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a4e6e5-5b15-4c9f-a1e9-a33081bf8588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8a2c860-253d-480f-aa78-ec5cd84143b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF with TOC-based chunking for Levels 2 & 3...\n",
      "✅ Found block for '1. General' at index 55 on page 10.\n",
      "✅ Found block for '2. Punctuation' at index 58 on page 10.\n",
      "✅ Found block for 'Full stop' at index 63 on page 10.\n",
      "✅ Found block for 'Colon' at index 79 on page 11.\n",
      "✅ Found block for 'Semicolon' at index 85 on page 11.\n",
      "✅ Found block for 'Comma' at index 97 on page 12.\n",
      "✅ Found block for 'Dashes' at index 170 on page 15.\n",
      "✅ Found block for 'Brackets' at index 179 on page 16.\n",
      "✅ Found block for 'Question mark' at index 195 on page 16.\n",
      "✅ Found block for 'Exclamation mark' at index 205 on page 17.\n",
      "✅ Found block for 'Quotation marks' at index 210 on page 17.\n",
      "✅ Found block for 'Forward slash' at index 241 on page 19.\n",
      "✅ Found block for 'Apostrophe' at index 248 on page 19.\n",
      "✅ Found block for '3. Spelling' at index 295 on page 21.\n",
      "✅ Found block for 'Conventions' at index 296 on page 21.\n",
      "✅ Found block for 'Interference effects' at index 334 on page 24.\n",
      "✅ Found block for 'Compound words and hyphens' at index 343 on page 25.\n",
      "✅ Found block for '4. Capitalisation' at index 423 on page 28.\n",
      "✅ Found block for '5. Names and titles' at index 602 on page 35.\n",
      "✅ Found block for 'Personal names and titles' at index 603 on page 35.\n",
      "✅ Found block for 'Names of bodies' at index 626 on page 37.\n",
      "✅ Found block for 'Names of ships, aircraft and other vehicles' at index 661 on page 38.\n",
      "✅ Found block for 'Geographical names' at index 674 on page 39.\n",
      "✅ Found block for '6. Numbers' at index 723 on page 42.\n",
      "✅ Found block for 'Writing out numbers' at index 724 on page 42.\n",
      "✅ Found block for 'Serial numbers' at index 786 on page 44.\n",
      "✅ Found block for 'Fractions and percentages' at index 810 on page 45.\n",
      "✅ Found block for 'Ranges' at index 822 on page 46.\n",
      "✅ Found block for 'Dates and times' at index 834 on page 47.\n",
      "✅ Found block for '7. Abbreviations, symbols and units of measurement' at index 903 on page 49.\n",
      "✅ Found block for 'Abbreviations' at index 904 on page 49.\n",
      "✅ Found block for 'Mathematical symbols' at index 981 on page 53.\n",
      "✅ Found block for 'Scientific symbols and units of measurement' at index 1001 on page 54.\n",
      "✅ Found block for '8. Currencies' at index 1026 on page 56.\n",
      "✅ Found block for '9. Foreign imports' at index 1046 on page 57.\n",
      "✅ Found block for 'Foreign words and phrases in English text' at index 1047 on page 57.\n",
      "✅ Found block for 'Romanisation systems' at index 1064 on page 58.\n",
      "✅ Found block for '10. Parts of speech' at index 1081 on page 59.\n",
      "✅ Found block for 'Adjectives and adverbs' at index 1082 on page 59.\n",
      "✅ Found block for 'Nouns: singular or plural' at index 1134 on page 61.\n",
      "✅ Found block for 'Nouns: people or persons' at index 1189 on page 63.\n",
      "✅ Found block for 'Verbs: present perfect/simple past' at index 1192 on page 63.\n",
      "✅ Found block for 'Verbs: tenses in minutes' at index 1202 on page 64.\n",
      "✅ Found block for 'Verbs: usage in legislation, contracts and the like' at index 1220 on page 64.\n",
      "✅ Found block for 'Verbs: split infinitive' at index 1286 on page 67.\n",
      "✅ Found block for 'Verbs: the -ing form and the possessive' at index 1295 on page 67.\n",
      "✅ Found block for 'Conjunctions' at index 1311 on page 68.\n",
      "✅ Found block for '11. Lists' at index 1325 on page 68.\n",
      "✅ Found block for '12. Legal language' at index 1362 on page 70.\n",
      "✅ Found block for '13. Footnotes, citations and references' at index 1378 on page 71.\n",
      "✅ Found block for '14. Correspondence' at index 1416 on page 73.\n",
      "✅ Found block for '15. Inclusive language' at index 1453 on page 74.\n",
      "✅ Found block for '16. Science guide' at index 1523 on page 77.\n",
      "✅ Found block for '17. The European Union' at index 1582 on page 83.\n",
      "✅ Found block for '18. Primary legislation' at index 1606 on page 84.\n",
      "✅ Found block for 'The treaties – an overview' at index 1608 on page 84.\n",
      "✅ Found block for 'The treaties in detail' at index 1631 on page 85.\n",
      "✅ Found block for 'Treaty citations' at index 1693 on page 88.\n",
      "✅ Found block for '19. Secondary legislation' at index 1711 on page 89.\n",
      "✅ Found block for 'Legislative procedures' at index 1721 on page 89.\n",
      "✅ Found block for 'Titles and numbering' at index 1727 on page 90.\n",
      "✅ Found block for 'Structure of acts' at index 1827 on page 94.\n",
      "✅ Found block for 'Referring to subdivisions of acts' at index 1871 on page 96.\n",
      "✅ Found block for '20. The EU institutions' at index 1884 on page 97.\n",
      "✅ Found block for 'Commission' at index 1885 on page 97.\n",
      "✅ Found block for 'Council' at index 1903 on page 98.\n",
      "✅ Found block for 'European Council' at index 1926 on page 99.\n",
      "✅ Found block for 'European Parliament' at index 1929 on page 99.\n",
      "✅ Found block for 'Court of Justice of the European Union' at index 1952 on page 100.\n",
      "✅ Found block for 'European Court of Auditors' at index 2018 on page 103.\n",
      "✅ Found block for 'European Economic and Social Committee' at index 2024 on page 103.\n",
      "✅ Found block for 'Committee of the Regions' at index 2034 on page 104.\n",
      "✅ Found block for 'European Central Bank' at index 2039 on page 104.\n",
      "✅ Found block for 'Other financial institutions' at index 2043 on page 104.\n",
      "✅ Found block for 'Interinstitutional bodies' at index 2051 on page 105.\n",
      "✅ Found block for 'Agencies' at index 2062 on page 105.\n",
      "✅ Found block for '21. References to official publications' at index 2065 on page 105.\n",
      "✅ Found block for 'The Official Journal' at index 2066 on page 105.\n",
      "✅ Found block for 'General Report and Bulletin' at index 2095 on page 106.\n",
      "✅ Found block for '22. EU finances' at index 2109 on page 107.\n",
      "✅ Found block for 'Budget' at index 2112 on page 107.\n",
      "✅ Found block for 'Programmes and funds financed from the EU budget and NextGenerationEU' at index 2132 on page 109.\n",
      "✅ Found block for 'Other funds' at index 2143 on page 110.\n",
      "✅ Found block for '23. Member States' at index 2151 on page 111.\n",
      "✅ Found block for 'Permanent Representations/Representatives' at index 2162 on page 111.\n",
      "✅ Found block for 'National parliaments' at index 2168 on page 112.\n",
      "✅ Found block for 'National judicial bodies' at index 2178 on page 112.\n",
      "✅ Found block for 'National legislation' at index 2181 on page 112.\n",
      "✅ Found block for '24. Official languages' at index 2239 on page 115.\n",
      "✅ Found block for '25. External relations' at index 2248 on page 116.\n",
      "✅ Found block for 'Annex 1 International organisations whose names do not follow our standard spelling rules' at index 2284 on page 120 using special rule.\n",
      "✅ Found block for 'Annex 2 Transliteration table for Greek' at index 2329 on page 122.\n",
      "✅ Found block for 'Annex 3 Conversion table for Greek serial numbering' at index 2377 on page 125.\n",
      "✅ Found block for 'Annex 4 Transliteration table for Cyrillic' at index 2390 on page 127.\n",
      "✅ Found block for 'Annex 5 Conversion table for Bulgarian serial numbering' at index 2471 on page 130.\n",
      "✅ Found block for 'Annex 6 Conversion table for numbering of inserted articles' at index 2476 on page 131.\n",
      "✅ Found block for 'Annex 7 Forms of address' at index 2534 on page 132.\n",
      "Generated 97 chunks\n",
      "Average tokens per chunk: 686.5\n",
      "Saved chunks to ../data/processed/toc_based_chunks_v18.json\n",
      "\n",
      "=== SAMPLE TOC CHUNKS V18 ===\n",
      "\n",
      "Section 1: 1. General\n",
      "Page: 10, Tokens: 227\n",
      "Text preview: 1. \n",
      "General\n",
      "1.1. \n",
      "Language usage. The language used in English texts should be understandable \n",
      "to speakers of Irish/British English (defined in the introduction to this guide as \n",
      "the shared standard usage of Ireland and the United Kingdom). As a general \n",
      "rule, Irish/British English should be preferred, and Americanisms that are liable \n",
      "not to be understood by speakers of Irish/British English should be avoided. \n",
      "However, bearing in mind that a considerable proportion of the target \n",
      "readership ma...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Section 2: 2. Punctuation\n",
      "Page: 10, Tokens: 108\n",
      "Text preview: 2. \n",
      "Punctuation\n",
      "2.1. \n",
      "The punctuation in an English text must follow the rules and conventions for \n",
      "English, which often differ from those applying to other languages. Note in \n",
      "particular that:\n",
      "♦ punctuation marks in English are always – apart from dashes (see 2.17) and\n",
      "ellipsis points (see 2.3) – closed up to the preceding word, letter or number;\n",
      "♦ stops (. ? ! : ;) are always followed by only a single (not a double) space....\n",
      "------------------------------------------------------------\n",
      "\n",
      "Section 3: Full stop\n",
      "Page: 10, Tokens: 456\n",
      "Text preview: Full stop\n",
      "2.2. \n",
      "A full stop marks the end of a sentence. All footnotes end with a full stop. Do \n",
      "not use a full stop at the end of a heading.\n",
      "No further full stop is required if a sentence ends with an ellipsis (…), with an \n",
      "abbreviation that takes a point (e.g. ‘etc.’) or with a quotation complete in itself \n",
      "that ends in a full stop, question mark or exclamation mark before the closing \n",
      "quote:\n",
      "Winston Churchill said: ‘A pessimist sees the difficulty in every opportunity; an \n",
      "optimist sees the o...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Section 4: Colon\n",
      "Page: 11, Tokens: 137\n",
      "Text preview: Colon\n",
      "2.5. \n",
      "A colon is most often used to indicate that an expansion, qualification, \n",
      "quotation or explanation is about to follow (e.g. a list of items in running text). \n",
      "The part before the colon must be a full sentence in its own right, but the \n",
      "second need not be.\n",
      "See also Chapter 11 for lists.\n",
      "2.6. \n",
      "Do not use colons at the end of headings.\n",
      "2.7. \n",
      "In Irish/British usage, colons do not require the next word to start with a \n",
      "capital.\n",
      "2.8. \n",
      "Colons should be closed up to the preceding word, lette...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Section 5: Semicolon\n",
      "Page: 11, Tokens: 222\n",
      "Text preview: Semicolon\n",
      "2.9. \n",
      "Use a semicolon rather than a comma to combine two sentences into one \n",
      "without a linking conjunction:\n",
      "\n",
      "1  \n",
      "In Word:  Alt + Ctrl + (full stop).\n",
      "2  \n",
      "In Windows: Alt + 0160. In Word: Ctrl + Shift + Space.\n",
      "14 February 2025 \n",
      "5/130\n",
      "English Style Guide\n",
      "The committee dealing with the question of commas agreed on a final text; the \n",
      "issue of semicolons was not considered.\n",
      "When items in a series are long and complex or involve internal punctuation, \n",
      "they should be separated by semicolons fo...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pymupdf as fitz\n",
    "import json\n",
    "import tiktoken\n",
    "import re\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "\n",
    "def get_toc_from_metadata(pdf_path: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Parses the document's TOC using PyMuPDF's get_toc() method, which extracts\n",
    "    the official table of contents from the PDF's metadata. This is the most\n",
    "    reliable method for getting the hierarchical structure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        # We need the detailed TOC with levels\n",
    "        toc_entries = doc.get_toc(simple=False)\n",
    "        doc.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening or getting TOC from PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "    return toc_entries\n",
    "\n",
    "def create_chunks_from_filtered_toc(pdf_path: str, levels_to_include: List[int]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Creates chunks based on a filtered list of TOC entries. This version is more\n",
    "    robust by finding the exact text block for each title and using those\n",
    "    blocks as chunk boundaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {e}. Cannot calculate token count.\")\n",
    "        tokenizer = None\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "    full_toc = get_toc_from_metadata(pdf_path)\n",
    "\n",
    "    # Filter for specified levels\n",
    "    filtered_toc = [entry for entry in full_toc if entry[0] in levels_to_include]\n",
    "    \n",
    "    # Store tuples of (title, page_number) for easier lookup\n",
    "    toc_boundaries = []\n",
    "    for level, title, page_num, _ in filtered_toc:\n",
    "        toc_boundaries.append((title, page_num))\n",
    "\n",
    "    # Get all text blocks from the document once to improve performance\n",
    "    all_blocks = []\n",
    "    for page_index, page in enumerate(doc):\n",
    "        page_blocks = page.get_text(\"blocks\", sort=True)\n",
    "        # Store page number along with each block\n",
    "        all_blocks.extend([(block, page_index + 1) for block in page_blocks])\n",
    "\n",
    "    # Now, find the index of the first text block for each TOC entry\n",
    "    boundary_block_indices = []\n",
    "    for title, page_num in toc_boundaries:\n",
    "        found_index = -1\n",
    "        normalized_title = re.sub(r'\\s+', ' ', title.strip())\n",
    "        for i, (block, p_num) in enumerate(all_blocks):\n",
    "            if p_num == page_num:\n",
    "                normalized_block_text = re.sub(r'\\s+', ' ', block[4].strip())\n",
    "                \n",
    "                # SPECIAL RULE for the tricky Annex 1 title\n",
    "                if \"Annex 1\" in normalized_title and normalized_block_text.startswith(\"Annex 1\"):\n",
    "                    boundary_block_indices.append(i)\n",
    "                    print(f\"✅ Found block for '{title}' at index {i} on page {page_num} using special rule.\")\n",
    "                    found_index = i\n",
    "                    break\n",
    "                \n",
    "                # Normal, strict matching for all other titles\n",
    "                if normalized_title == normalized_block_text:\n",
    "                    boundary_block_indices.append(i)\n",
    "                    print(f\"✅ Found block for '{title}' at index {i} on page {page_num}.\")\n",
    "                    found_index = i\n",
    "                    break\n",
    "        if found_index == -1:\n",
    "            print(f\"❌ Failed to find block for '{title}' on page {page_num}. This entry will be skipped.\")\n",
    "\n",
    "    # Sort the indices to process them in document order\n",
    "    boundary_block_indices.sort()\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # Iterate through the boundary indices to define chunk content\n",
    "    for i in range(len(boundary_block_indices)):\n",
    "        start_index = boundary_block_indices[i]\n",
    "        end_index = boundary_block_indices[i+1] if i + 1 < len(boundary_block_indices) else len(all_blocks)\n",
    "        \n",
    "        # Get the blocks for the current chunk\n",
    "        chunk_blocks = all_blocks[start_index:end_index]\n",
    "        \n",
    "        # Extract title and page from the starting block\n",
    "        start_block, start_page = all_blocks[start_index]\n",
    "        chunk_title = re.sub(r'\\s+', ' ', start_block[4].strip())\n",
    "        \n",
    "        # Combine the text from all blocks in the chunk\n",
    "        full_chunk_text = \"\\n\".join([block[4].strip() for block, _ in chunk_blocks])\n",
    "        \n",
    "        # Calculate token count\n",
    "        token_count = len(tokenizer.encode(full_chunk_text)) if tokenizer else None\n",
    "\n",
    "        chunks.append({\n",
    "            'chunk_id': f'toc_chunk_{len(chunks):04d}',\n",
    "            'text': full_chunk_text,\n",
    "            'page_number': start_page,\n",
    "            'token_count': token_count,\n",
    "            'method': 'filtered_toc_based',\n",
    "            'section_title': chunk_title\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "    return chunks\n",
    "\n",
    "# Test the function with the user's PDF\n",
    "pdf_path = \"../data/raw/English_Style_Guide-European_Commission.pdf\"\n",
    "\n",
    "print(\"Processing PDF with TOC-based chunking for Levels 2 & 3...\")\n",
    "filtered_toc_chunks = create_chunks_from_filtered_toc(pdf_path, levels_to_include=[2, 3])\n",
    "\n",
    "print(f\"Generated {len(filtered_toc_chunks)} chunks\")\n",
    "\n",
    "if filtered_toc_chunks:\n",
    "    total_tokens = sum(c['token_count'] for c in filtered_toc_chunks if c['token_count'])\n",
    "    num_valid_chunks = len([c for c in filtered_toc_chunks if c['token_count']])\n",
    "    if num_valid_chunks > 0:\n",
    "        avg_tokens = total_tokens / num_valid_chunks\n",
    "        print(f\"Average tokens per chunk: {avg_tokens:.1f}\")\n",
    "    \n",
    "    # Save results to a file for review\n",
    "    output_file = '../data/processed/toc_based_chunks_v18.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_toc_chunks, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved chunks to {output_file}\")\n",
    "\n",
    "    # Display sample results\n",
    "    print(\"\\n=== SAMPLE TOC CHUNKS V18 ===\")\n",
    "    for i, chunk in enumerate(filtered_toc_chunks[:5]):\n",
    "        print(f\"\\nSection {i+1}: {chunk['section_title']}\")\n",
    "        print(f\"Page: {chunk['page_number']}, Tokens: {chunk['token_count']}\")\n",
    "        print(f\"Text preview: {chunk['text'][:500]}...\")\n",
    "        print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b91631-6992-42cf-aad2-41558aede3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c702f83-4b2e-438b-810b-37efefc73077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d77b2a-8d4c-48dc-996a-96a6b6ee5ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be644780-3964-4e60-bc98-d3831c854f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1635e6f1-4bd3-4138-ac53-303941e567a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc31bf-99b4-4591-91a5-7ca6919da6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
