{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6ae601-971c-46a0-b600-d1a1cde8aa4b",
   "metadata": {},
   "source": [
    "# **Building a RAG Knowledge Assistant for Translators**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983345a-009d-4b08-9766-40541b476ab2",
   "metadata": {},
   "source": [
    "## **Project Summary**\n",
    "\n",
    "In this project, I explain the process of building a Retrieval-Augmented Generation (RAG) system, from initial experimentation to a final, deployed application. The system acts as a knowledge assistant for professional translators, providing accurate answers from the European Commission's English Style Guide. This work explores how key component choices contribute to a more effective pipeline, focusing on three central questions:\n",
    "\n",
    "1.  What is the impact of different **chunking strategies** on the quality of the retrieval results?\n",
    "2.  Which **retrieval method** (keyword, semantic, or hybrid) yields the most relevant information?\n",
    "3.  How can **prompt engineering** reduce hallucinations and improve the final answer's reliability?\n",
    "\n",
    "The insights from these experiments resulted in a practical web application built with **Streamlit**. The entire system, which uses **Amazon Bedrock** for LLM access, was containerized with **Docker** and deployed on an **Amazon EC2** instance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8216f-ba3b-481e-8189-161ecdbf5aa4",
   "metadata": {},
   "source": [
    "## **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2738fc0-d992-49e5-a09b-65617ed7fc9b",
   "metadata": {},
   "source": [
    "**RAG** has gained popularity in recent years, and it's easy to see why. It bridges a critical gap between large language models **(LLMs)** and traditional **semantic search**, providing a more efficient way to get accurate, up-to-date answers from documents.\n",
    "\n",
    "Here’s the problem with **LLMs** alone: they are great at generating fluent, coherent responses, but they rely entirely on their **training data**. If you ask about something not in that data, like a new style guide update or an internal document, they can’t help. You’d need to fine-tune the model, which is expensive, time-consuming, and hard to maintain.\n",
    "\n",
    "Even if you paste the relevant text into the prompt, you run into other issues. The **context window** limits how much you can include. And LLMs can still hallucinate, giving confident but wrong answers even when the correct information is right there in the prompt.\n",
    "\n",
    "Traditional **semantic search** solves part of this. It works well when you need to find a document or passage based on keyword or meaning similarity. It retrieves what exists, but it **doesn’t generate natural language answers**. It gives you a list of results, and you still have to read through them to find what you need.\n",
    "\n",
    "**RAG enhances retrieval with generation**. It first searches your knowledge base for the most relevant chunks of text. Then it passes those to the LLM to generate a clear, concise answer, grounded in actual source material.\n",
    "\n",
    "The **pros**?  \n",
    "- You don’t need to retrain the model when your documents change. Just update the vector database.  \n",
    "- Answers are traceable. You can check which part of the document was used.  \n",
    "- It handles long, complex documents by breaking them into searchable pieces.  \n",
    "- You reduce hallucinations because the model only answers from retrieved content.\n",
    "\n",
    "The **cons**?  \n",
    "- Chunking matters. Bad splits can break context and hurt retrieval.  \n",
    "- Embeddings aren’t perfect. Sometimes the system misses relevant sections just because of phrasing differences.  \n",
    "- You still need safeguards, like prompt engineering, to keep the LLM honest and on track.\n",
    "\n",
    "In short: RAG gives you a flexible, cost-effective way to build question-answering systems over your own documents, without retraining models or trusting blind guesses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f9b070-4399-448e-b8e0-b409717693ae",
   "metadata": {},
   "source": [
    "## **2. Notebook Setup and Configuration**\n",
    "\n",
    "### **2.1. The Source Document**\n",
    "\n",
    "For the source material, I chose the **European Commission’s English Style Guide**. I specifically picked a **PDF** because while they are extremely common in professional environments, they can be notoriously **difficult to process**. PDFs are designed for visual consistency, not for machine readability, which can make extracting text and understanding its underlying structure a significant engineering challenge.\n",
    "\n",
    "This particular guide adds another layer of complexity with its deep **hierarchy** of chapters, sections, and subsections. Instead of starting with a clean text file, I chose this document to tackle a realistic problem head-on: how do you build a high-quality RAG system when your source material is messy? The goal is to show that even with a difficult format, we can develop a parsing strategy that captures the document's structure to create meaningful chunks for retrieval.\n",
    "\n",
    "### **2.2. Libraries**\n",
    "\n",
    "This first step handles the necessary imports and sets up the main configuration variables for the project.\n",
    "\n",
    "A few key libraries were used here:\n",
    "* **`pymupdf`**: A powerful library for extracting text, images, and metadata from PDF documents. We'll use it to get the raw text content from each page.\n",
    "* **`tiktoken`**: This is OpenAI's tokenizer. We use it to count tokens accurately, which is essential for ensuring our text chunks don't exceed the context window of the embedding model.\n",
    "* **`RecursiveCharacterTextSplitter`** from LangChain: A practical tool for breaking text into smaller pieces while trying to preserve natural boundaries like paragraphs and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25bea8c3-7673-4228-a6ab-1baaf1552316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS AND CONFIGURATION ===\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import pymupdf as fitz\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Main Configuration ---\n",
    "config = {\n",
    "    'data_dir': '../data/',\n",
    "    'pdf_file_name': 'English_Style_Guide-European_Commission.pdf'\n",
    "}\n",
    "\n",
    "# --- Path Definitions ---\n",
    "PDF_PATH = f\"{config['data_dir']}raw/{config['pdf_file_name']}\"\n",
    "FIXED_CHUNK_PATH = f\"{config['data_dir']}processed/fixed_chunks.json\"\n",
    "SECTION_CHUNK_PATH = f\"{config['data_dir']}processed/section_chunks.json\"\n",
    "\n",
    "# --- Document Metadata ---\n",
    "SOURCE_DOCUMENT = config['pdf_file_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e9f9b6-3853-4352-842f-51974e084427",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59d7bc-a9f7-4b3a-82a6-34683a8ab53d",
   "metadata": {},
   "source": [
    "## **3. PDF Processing and Chunking Strategies**\n",
    "\n",
    "We’re now ready to extract text from the PDF and split it into chunks for retrieval. The quality of these chunks is one of the most critical factors for a RAG system's success. To answer the first key question of this project—*what is the impact of different chunking strategies on the quality of the retrieval results?*—I will implement and compare two distinct methods:\n",
    "\n",
    "-  **Fixed-Size Chunking**: This approach splits text into relatively uniform size chunks. It’s fast and simple, but sometimes ignores the document’s structure, splitting paragraphs or ideas across different chunks. I’ll use it as a baseline.\n",
    "-  **Section-Based Chunking**: This is a more advanced, layout-aware strategy that uses the document's visual hierarchy (chapters, sections, etc.) to create more meaningful and contextually complete chunks.\n",
    "\n",
    "To implement these strategies effectively, we first need to prepare the source material by cleaning the raw text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb4536-2f96-4a9d-bd3b-c32c2beead2f",
   "metadata": {},
   "source": [
    "### **3.1. Data Cleaning**\n",
    "\n",
    "Raw text extracted from a PDF tends to be far from perfect. It often includes repetitive headers, footers, page numbers, and other digital artifacts that confuse the embedding model and hurt retrieval accuracy. Before chunking the text, it’s important to remove this noise to ensure the quality of the data we feed into the model.\n",
    "\n",
    "The cleaning function below uses a series of regular expressions to remove specific, recurring patterns identified during the initial exploration:\n",
    "* **Dates**: Removes the date that appears in the footer of the pages (\"14 February 2025\").\n",
    "* **Page Numbers**: Strips out the page numbering format (e.g., \"3/130\").\n",
    "* **Document Title**: Removes the \"English Style Guide\" text, which is part of the header.\n",
    "* **Software-Specific Notes**: Deletes certain instructional footnotes related to Microsoft Word or Windows.\n",
    "* **Extra Blank Lines**: Collapses multiple blank lines into a single one to keep the text formatting clean and consistent.\n",
    "\n",
    "This function will be applied to all text chunks to improve their quality by removing this recurring noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43cd3ed-4e92-44b2-9b97-1f040cc2fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA CLEANING FUNCTION ===\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Removes recurring headers, footers, and other PDF artifacts from text.\"\"\"\n",
    "    text = re.sub(r'14 February 2025', '', text)\n",
    "    text = re.sub(r'(\\n)?\\d{1,3}/\\d{3}(\\n)?', '', text)\n",
    "    text = re.sub(r'(\\n)?English Style Guide(\\n)?', '', text)\n",
    "    text = re.sub(r'\\n\\d+\\s+In (Word|Windows):.*\\n', '\\n', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decfb0c8-8c30-4f37-aafa-1caf6f981e06",
   "metadata": {},
   "source": [
    "### **3.2. Fixed-size Chunking**\n",
    "\n",
    "The first strategy is fixed-size chunking, a baseline approach where every chunk is constrained by a fixed maximum size (in this case, 256 tokens). Despite the strict size constraint, the method is smarter than a simple brute-force split. It uses LangChain's `RecursiveCharacterTextSplitter`, which doesn’t understand meaning but follows a smart splitting order, preferring paragraph and sentence breaks over random cuts. While the 256-token limit is the rule, the splitter does its best to respect the text's structure within that constraint.\n",
    "\n",
    "The process is handled in a few steps:\n",
    "1.  **Text Extraction**: First, the raw text is extracted from the PDF, starting at **page 6** to skip the cover, table of contents, and other front matter.\n",
    "2.  **Cleaning**: The extracted text is passed through the `clean_text` function to remove recurring headers, footers, and other digital artifacts.\n",
    "3.  **Splitting**: The `RecursiveCharacterTextSplitter` then generates the chunks based on two key parameters:\n",
    "    * **`max_tokens = 256`**: This is a typical chunk size for embedding models. For this document, it also creates chunks that are roughly comparable in size to the average logical section, allowing for a fairer comparison against the section-based strategy.\n",
    "    * **`overlap_tokens = 50`**: A 50-token overlap (about 20% of the chunk size) is used to help maintain context between adjacent chunks, reducing the chance that an idea is awkwardly cut off at a boundary.\n",
    "4.  **Metadata Assignment**: Finally, each chunk is packaged into a dictionary with important metadata, like its `chunk_id` and the method used.\n",
    "\n",
    "This strategy is fast and easy to implement, making it a solid baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602f7592-bf21-4d31-aae5-9afc8c2c3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FIXED-SIZE CHUNKING FUNCTION ===\n",
    "def create_fixed_size_chunks(pdf_path: str, max_tokens: int = 256, overlap_tokens: int = 50) -> List[Dict]:\n",
    "    \"\"\"Creates fixed-size text chunks as a baseline chunking strategy.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    # Extract text from main content pages, skipping front matter.\n",
    "    full_text = ''.join(page.get_text() for page in doc.pages(start=6))\n",
    "    doc.close()\n",
    "    \n",
    "    cleaned_text = clean_text(full_text)\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_tokens,\n",
    "        chunk_overlap=overlap_tokens,\n",
    "        length_function=lambda x: len(tokenizer.encode(x)),\n",
    "        separators=['\\n\\n', '\\n', '. ', ' ', ''],\n",
    "    )\n",
    "    texts = text_splitter.split_text(cleaned_text)\n",
    "    \n",
    "    chunks = []\n",
    "    for i, text in enumerate(texts):\n",
    "        token_count = len(tokenizer.encode(text))\n",
    "        chunks.append({\n",
    "            'chunk_id': f'fixed_{i:04d}',\n",
    "            'text': text,\n",
    "            'token_count': token_count,\n",
    "            'page_number': 'N/A',\n",
    "            'method': 'fixed_size',\n",
    "            'source_document': SOURCE_DOCUMENT\n",
    "        })\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed94dc5-1ef4-4eb2-b99d-bedf62dea86b",
   "metadata": {},
   "source": [
    "### **3.3. Section-Based Chunking**\n",
    "\n",
    "The second strategy is a more advanced, layout-aware approach designed to create semantically complete chunks. Instead of a relatively fixed size, the boundaries of these chunks are determined by the document's logical hierarchy, after identifying the headers for Parts, Chapters, and Sections.\n",
    "\n",
    "#### **3.3.1. Investigating the PDF Structure**\n",
    "\n",
    "Before splitting the document, we need to understand how those sections are defined by their formatting and text patterns. The script below uses `PyMuPDF` to inspect a few pages of the document. The code iterates through text \"spans\" and prints their font size, whether they are bold, and the text itself.\n",
    "\n",
    "This investigative step is crucial. By looking at the output, we can spot the patterns that define the document's hierarchy. For instance, we can see that chapter titles consistently use a specific font size and style, which is different from section or subsection titles.\n",
    "\n",
    "Here is the code I used for the inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c341f477-7041-43cd-8ce9-3f1fac53ddd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting pages [9, 10, 11, 12] ---\n",
      "\n",
      "--- PAGE 10 ---\n",
      "Size: 9.0, Bold: False, Text: 'English Style Guide'\n",
      "Size: 9.0, Bold: False, Text: '4/130'\n",
      "Size: 9.0, Bold: False, Text: '14 February 2025'\n",
      "Size: 14.04, Bold: True, Text: '1.'\n",
      "Size: 14.04, Bold: True, Text: 'General'\n",
      "Size: 12.0, Bold: False, Text: '1.1.'\n",
      "Size: 12.0, Bold: False, Text: 'Language usage.'\n",
      "Size: 12.0, Bold: False, Text: 'The language used in English texts should be understandable'\n",
      "Size: 12.0, Bold: False, Text: 'to speakers of Irish/British English (defined in the introduction to this guide as'\n",
      "Size: 12.0, Bold: False, Text: 'the shared standard usage of Ireland and the United Kingdom). As a general'\n",
      "Size: 12.0, Bold: False, Text: 'rule, Irish/British English should be preferred, and Americanisms that are liable'\n",
      "Size: 12.0, Bold: False, Text: 'not to be understood by speakers of Irish/British English should be avoided.'\n",
      "Size: 12.0, Bold: False, Text: 'However, bearing in mind that a considerable proportion of the target'\n",
      "Size: 12.0, Bold: False, Text: 'readership may be made up of non-native speakers, very colloquial'\n",
      "Size: 12.0, Bold: False, Text: 'Irish/British usage should also be avoided.'\n",
      "Size: 12.0, Bold: False, Text: '1.2.'\n",
      "Size: 12.0, Bold: False, Text: 'Quoting text.'\n",
      "Size: 12.0, Bold: False, Text: 'When directly quoting a piece of text or citing the title of a'\n",
      "Size: 12.0, Bold: False, Text: 'document, you should reproduce the original rather than following the'\n",
      "Size: 12.0, Bold: False, Text: 'conventions set out below. However, you should make it clear you are quoting'\n",
      "Size: 12.0, Bold: False, Text: 'text by putting it in quotation marks or italics or setting it off in some other'\n",
      "Size: 12.0, Bold: False, Text: 'way. If necessary, you may mark errors with ‘['\n",
      "Size: 12.0, Bold: False, Text: 'sic'\n",
      "Size: 12.0, Bold: False, Text: ']’ or insert missing text in'\n",
      "Size: 12.0, Bold: False, Text: 'square brackets.'\n",
      "Size: 14.04, Bold: True, Text: '2.'\n",
      "Size: 14.04, Bold: True, Text: 'Punctuation'\n",
      "Size: 12.0, Bold: False, Text: '2.1.'\n",
      "Size: 12.0, Bold: False, Text: 'The punctuation in an English text must follow the rules and conventions for'\n",
      "Size: 12.0, Bold: False, Text: 'English, which often differ from those applying to other languages. Note in'\n",
      "Size: 12.0, Bold: False, Text: 'particular that:'\n",
      "Size: 12.0, Bold: False, Text: '♦'\n",
      "Size: 12.0, Bold: False, Text: 'punctuation marks in English are always – apart from dashes (see'\n",
      "Size: 12.0, Bold: False, Text: '2.17'\n",
      "Size: 12.0, Bold: False, Text: ') and'\n",
      "Size: 12.0, Bold: False, Text: 'ellipsis points (see'\n",
      "Size: 12.0, Bold: False, Text: '2.3'\n",
      "Size: 12.0, Bold: False, Text: ') – closed up to the preceding word, letter or number;'\n",
      "Size: 12.0, Bold: False, Text: '♦'\n",
      "Size: 12.0, Bold: False, Text: 'stops (. ? ! : ;) are always followed by only a single (not a double) space.'\n",
      "Size: 12.0, Bold: True, Text: 'Full stop'\n",
      "Size: 12.0, Bold: False, Text: '2.2.'\n",
      "Size: 12.0, Bold: False, Text: 'A full stop marks the end of a sentence. All footnotes end with a full stop. Do'\n",
      "Size: 12.0, Bold: False, Text: 'not use a full stop at the end of a heading.'\n",
      "Size: 12.0, Bold: False, Text: 'No further full stop is required if a sentence ends with an ellipsis (…), with an'\n",
      "Size: 12.0, Bold: False, Text: 'abbreviation that takes a point (e.g. ‘etc.’) or with a quotation complete in itself'\n",
      "Size: 12.0, Bold: False, Text: 'that ends in a full stop, question mark or exclamation mark before the closing'\n",
      "Size: 12.0, Bold: False, Text: 'quote:'\n",
      "Size: 9.96, Bold: False, Text: 'Winston Churchill said: ‘A pessimist sees the difficulty in every opportunity; an'\n",
      "Size: 9.96, Bold: False, Text: 'optimist sees the opportunity in every difficulty.’'\n",
      "Size: 12.0, Bold: False, Text: 'Truncations (in which the end of the word is deleted) are followed by a point'\n",
      "Size: 12.0, Bold: False, Text: '(for example'\n",
      "Size: 12.0, Bold: False, Text: 'Co.'\n",
      "Size: 12.0, Bold: False, Text: ','\n",
      "Size: 12.0, Bold: False, Text: 'Art.'\n",
      "Size: 12.0, Bold: False, Text: ','\n",
      "Size: 12.0, Bold: False, Text: 'Chap.'\n",
      "Size: 12.0, Bold: False, Text: '), but contractions (in which the middle of the'\n",
      "Size: 12.0, Bold: False, Text: 'word is removed) are not (for example'\n",
      "Size: 12.0, Bold: False, Text: 'Dr'\n",
      "Size: 12.0, Bold: False, Text: ','\n",
      "Size: 12.0, Bold: False, Text: 'Ms'\n",
      "Size: 12.0, Bold: False, Text: ','\n",
      "Size: 12.0, Bold: False, Text: 'Ltd'\n",
      "Size: 12.0, Bold: False, Text: '). See also'\n",
      "Size: 12.0, Bold: False, Text: '7.2'\n",
      "Size: 12.0, Bold: False, Text: '.'\n",
      "\n",
      "--- PAGE 11 ---\n",
      "Size: 9.0, Bold: False, Text: 'English Style Guide'\n",
      "Size: 9.0, Bold: False, Text: '14 February 2025'\n",
      "Size: 9.0, Bold: False, Text: '5/130'\n",
      "Size: 12.0, Bold: False, Text: '2.3.'\n",
      "Size: 12.0, Bold: False, Text: 'Ellipsis.'\n",
      "Size: 12.0, Bold: False, Text: 'An ellipsis'\n",
      "Size: 8.04, Bold: False, Text: '1'\n",
      "Size: 12.0, Bold: False, Text: 'is three points indicating an omission in the text. If an'\n",
      "Size: 12.0, Bold: False, Text: 'ellipsis falls at the end of a sentence, there is no final full stop. However, if'\n",
      "Size: 12.0, Bold: False, Text: 'followed by another punctuation mark (e.g. question mark, colon, semicolon or'\n",
      "Size: 12.0, Bold: False, Text: 'quotation mark), the punctuation mark should be closed up to the ellipsis.'\n",
      "Size: 12.0, Bold: False, Text: 'When placed at the beginning of the text, it is followed by a normal space.'\n",
      "Size: 12.0, Bold: False, Text: 'When replacing one or more words in the middle of a sentence, it is preceded'\n",
      "Size: 12.0, Bold: False, Text: 'by a hard space'\n",
      "Size: 8.04, Bold: False, Text: '2'\n",
      "Size: 12.0, Bold: False, Text: 'and followed by a normal space.'\n",
      "Size: 12.0, Bold: False, Text: 'When replacing one or more words at the end of a sentence, it is preceded by a'\n",
      "Size: 12.0, Bold: False, Text: 'hard space'\n",
      "Size: 8.04, Bold: False, Text: '2'\n",
      "Size: 12.0, Bold: False, Text: '.'\n",
      "Size: 12.0, Bold: False, Text: 'The points are not enclosed in brackets:'\n",
      "Size: 9.96, Bold: False, Text: '‘The objectives of the Union shall be achieved … while respecting the principle'\n",
      "Size: 9.96, Bold: False, Text: 'of subsidiarity.’'\n",
      "Size: 12.0, Bold: False, Text: 'However, where a line or paragraph is omitted and replaced by an ellipsis, the'\n",
      "Size: 12.0, Bold: False, Text: 'ellipsis should be placed within square brackets on a separate line.'\n",
      "Size: 12.0, Bold: False, Text: 'Do not use an ellipsis to replace or reinforce the word ‘etc.’'\n",
      "Size: 12.0, Bold: False, Text: '2.4.'\n",
      "Size: 12.0, Bold: False, Text: 'Run-in side heads (you are looking at one)'\n",
      "Size: 12.0, Bold: False, Text: '. These are followed by a full stop,'\n",
      "Size: 12.0, Bold: False, Text: 'not a colon.'\n",
      "Size: 12.0, Bold: True, Text: 'Colon'\n",
      "Size: 12.0, Bold: False, Text: '2.5.'\n",
      "Size: 12.0, Bold: False, Text: 'A colon is most often used to indicate that an expansion, qualification,'\n",
      "Size: 12.0, Bold: False, Text: 'quotation or explanation is about to follow (e.g. a list of items in running text).'\n",
      "Size: 12.0, Bold: False, Text: 'The part before the colon must be a full sentence in its own right, but the'\n",
      "Size: 12.0, Bold: False, Text: 'second need not be.'\n",
      "Size: 12.0, Bold: False, Text: 'See also Chapter'\n",
      "Size: 12.0, Bold: False, Text: '11'\n",
      "Size: 12.0, Bold: False, Text: 'for'\n",
      "Size: 12.0, Bold: False, Text: 'lists'\n",
      "Size: 12.0, Bold: False, Text: '.'\n",
      "Size: 12.0, Bold: False, Text: '2.6.'\n",
      "Size: 12.0, Bold: False, Text: 'Do not use colons at the end of headings.'\n",
      "Size: 12.0, Bold: False, Text: '2.7.'\n",
      "Size: 12.0, Bold: False, Text: 'In Irish/British usage, colons do not require the next word to start with a'\n",
      "Size: 12.0, Bold: False, Text: 'capital.'\n",
      "Size: 12.0, Bold: False, Text: '2.8.'\n",
      "Size: 12.0, Bold: False, Text: 'Colons should be closed up to the preceding word, letter or number.'\n",
      "Size: 12.0, Bold: True, Text: 'Semicolon'\n",
      "Size: 12.0, Bold: False, Text: '2.9.'\n",
      "Size: 12.0, Bold: False, Text: 'Use a semicolon rather than a comma to combine two sentences into one'\n",
      "Size: 12.0, Bold: False, Text: 'without a linking conjunction:'\n",
      "Size: 8.04, Bold: False, Text: '1'\n",
      "Size: 9.96, Bold: False, Text: 'In Word:  Alt + Ctrl + (full stop).'\n",
      "Size: 8.04, Bold: False, Text: '2'\n",
      "Size: 9.96, Bold: False, Text: 'In Windows: Alt + 0160. In Word: Ctrl + Shift + Space.'\n",
      "\n",
      "--- PAGE 12 ---\n",
      "Size: 9.0, Bold: False, Text: 'English Style Guide'\n",
      "Size: 9.0, Bold: False, Text: '6/130'\n",
      "Size: 9.0, Bold: False, Text: '14 February 2025'\n",
      "Size: 9.96, Bold: False, Text: 'The committee dealing with the question of commas agreed on a final text; the'\n",
      "Size: 9.96, Bold: False, Text: 'issue of semicolons was not considered.'\n",
      "Size: 12.0, Bold: False, Text: 'When items in a series are long and complex or involve internal punctuation,'\n",
      "Size: 12.0, Bold: False, Text: 'they should be separated by semicolons for the sake of clarity:'\n",
      "Size: 9.96, Bold: False, Text: 'The membership of the international commission was as follows: France, 4,'\n",
      "Size: 9.96, Bold: False, Text: 'which had 3 members until 2010; Germany, 5, whose membership remained'\n",
      "Size: 9.96, Bold: False, Text: 'stable; and Italy, 3, whose membership increased from 1 in 2001.'\n",
      "Size: 12.0, Bold: False, Text: 'See also Chapter'\n",
      "Size: 12.0, Bold: False, Text: '11'\n",
      "Size: 12.0, Bold: False, Text: 'for the use of'\n",
      "Size: 12.0, Bold: False, Text: 'semicolons in lists'\n",
      "Size: 12.0, Bold: False, Text: '.'\n",
      "Size: 12.0, Bold: False, Text: '2.10.'\n",
      "Size: 12.0, Bold: False, Text: 'Semicolons should be closed up to the preceding word, letter or number.'\n",
      "Size: 12.0, Bold: True, Text: 'Comma'\n",
      "Size: 12.0, Bold: False, Text: '2.11.'\n",
      "Size: 12.0, Bold: False, Text: 'Items in a series'\n",
      "Size: 12.0, Bold: False, Text: '. In a list of two items, these are separated by ‘and’ or ‘or’:'\n",
      "Size: 9.96, Bold: False, Text: 'The committee identified two errors in the document: the date of implementation'\n",
      "Size: 9.96, Bold: False, Text: 'and the regulation number.'\n",
      "Size: 12.0, Bold: False, Text: 'In a list of three or more items, a comma is used to separate them, except for'\n",
      "Size: 12.0, Bold: False, Text: 'the final two, which are separated by ‘and’ or ‘or’:'\n",
      "Size: 9.96, Bold: False, Text: 'Robin mowed the lawn, Sam did the cooking and Kim lazed around.'\n",
      "Size: 9.96, Bold: False, Text: 'The committee considered sugar, beef and milk products.'\n",
      "Size: 12.0, Bold: False, Text: 'An additional comma before the final item is sometimes essential to help'\n",
      "Size: 12.0, Bold: False, Text: 'clarify the sense. Compare the examples below:'\n",
      "Size: 9.96, Bold: False, Text: 'X may not be added to beef, ham or processed meat and milk products'\n",
      "Size: 9.96, Bold: False, Text: '[unclear]'\n",
      "Size: 9.96, Bold: False, Text: 'The use of X is forbidden in beef, ham or processed meat, and milk products'\n",
      "Size: 12.0, Bold: False, Text: 'A comma also comes before ‘etc.’ in a series:'\n",
      "Size: 9.96, Bold: False, Text: 'sugar, beef, milk products, etc.'\n",
      "Size: 12.0, Bold: False, Text: 'but not if no series is involved:'\n",
      "Size: 9.96, Bold: False, Text: 'They discussed milk products etc., then moved on to sugar.'\n",
      "Size: 12.0, Bold: False, Text: 'Commas also divide adjectives in series:'\n",
      "Size: 9.96, Bold: False, Text: 'moderate, stable prices'\n",
      "Size: 9.96, Bold: False, Text: 'dry, fruity wine'\n",
      "Size: 12.0, Bold: False, Text: 'but not if the adjectives do not form a series:'\n",
      "Size: 9.96, Bold: False, Text: 'stable agricultural prices'\n",
      "Size: 9.96, Bold: False, Text: 'sweet red wine'\n",
      "Size: 12.0, Bold: False, Text: 'The adjectives in the first pair of examples are coordinate adjectives. They'\n",
      "Size: 12.0, Bold: False, Text: 'separately describe the noun that follows them. They could be inverted or'\n",
      "Size: 12.0, Bold: False, Text: 'conjoined by ‘and’.'\n",
      "\n",
      "--- PAGE 13 ---\n",
      "Size: 9.0, Bold: False, Text: 'English Style Guide'\n",
      "Size: 9.0, Bold: False, Text: '14 February 2025'\n",
      "Size: 9.0, Bold: False, Text: '7/130'\n",
      "Size: 12.0, Bold: False, Text: 'The adjectives in the second pair of examples are cumulative adjectives.'\n",
      "Size: 12.0, Bold: False, Text: '‘Agricultural prices’ and ‘red wine’ form a lexical unit that is described by the'\n",
      "Size: 12.0, Bold: False, Text: 'adjective that precedes them. They cannot be inverted or conjoined by ‘and’.'\n",
      "Size: 12.0, Bold: False, Text: '2.12.'\n",
      "Size: 12.0, Bold: False, Text: 'Linked clauses.'\n",
      "Size: 12.0, Bold: False, Text: 'Use a comma to separate two clauses linked by a conjunction'\n",
      "Size: 12.0, Bold: False, Text: 'such as ‘but’, ‘yet’, ‘while’ or ‘so’ to form a single sentence:'\n",
      "Size: 9.96, Bold: False, Text: 'The committee on commas agreed a final text, but the issue of semicolons was'\n",
      "Size: 9.96, Bold: False, Text: 'not considered.'\n",
      "Size: 12.0, Bold: False, Text: 'If the subject of the second clause is omitted, or if the conjunction is ‘and’, ‘or’'\n",
      "Size: 12.0, Bold: False, Text: 'or ‘but’, the comma is not obligatory:'\n",
      "Size: 9.96, Bold: False, Text: 'The committee on commas agreed a final text but did not consider the issue of'\n",
      "Size: 9.96, Bold: False, Text: 'semicolons.'\n",
      "Size: 9.96, Bold: False, Text: 'The committee on commas agreed a final text and the Council approved it.'\n",
      "Size: 12.0, Bold: False, Text: 'Where there is no conjunction, use a semicolon (see'\n",
      "Size: 12.0, Bold: False, Text: '2.9'\n",
      "Size: 12.0, Bold: False, Text: '):'\n",
      "Size: 9.96, Bold: False, Text: 'The committee dealing with the question of commas agreed on a final text; the'\n",
      "Size: 9.96, Bold: False, Text: 'issue of semicolons was not considered.'\n",
      "Size: 12.0, Bold: False, Text: '2.13.'\n",
      "Size: 12.0, Bold: False, Text: 'Setting off phrases and clauses.'\n",
      "Size: 12.0, Bold: False, Text: 'When a phrase introducing or adding to the'\n",
      "Size: 12.0, Bold: False, Text: 'information in a sentence has a separate emphasis of its own, it is set off by a'\n",
      "Size: 12.0, Bold: False, Text: 'comma:'\n",
      "Size: 9.96, Bold: False, Text: 'Mindful of the need to fudge the issue, the committee on commas decided to take'\n",
      "Size: 9.96, Bold: False, Text: 'no action.'\n",
      "Size: 9.96, Bold: False, Text: 'The committee on commas is composed of old fogeys, as we all know.'\n",
      "Size: 12.0, Bold: False, Text: 'If the phrase is placed in the middle of the sentence, it is set off by a pair of'\n",
      "Size: 12.0, Bold: False, Text: 'commas – or possibly dashes (see'\n",
      "Size: 12.0, Bold: False, Text: '2.17'\n",
      "Size: 12.0, Bold: False, Text: ') or brackets (see'\n",
      "Size: 12.0, Bold: False, Text: '2.19'\n",
      "Size: 12.0, Bold: False, Text: '):'\n",
      "Size: 9.96, Bold: False, Text: 'The committee on commas, discussing the issue for the 10th time, was still'\n",
      "Size: 9.96, Bold: False, Text: 'unable to reach agreement.'\n",
      "Size: 12.0, Bold: False, Text: 'The sentence must remain a complete sentence even if the introductory or'\n",
      "Size: 12.0, Bold: False, Text: 'inserted phrase is omitted.'\n",
      "Size: 12.0, Bold: False, Text: 'Short introductory phrases (typically two to three words) may be run into the'\n",
      "Size: 12.0, Bold: False, Text: 'rest of the sentence:'\n",
      "Size: 9.96, Bold: False, Text: 'In 2015 the committee took three decisions.'\n",
      "Size: 12.0, Bold: False, Text: 'No comma is required between a main clause and a subordinate clause when'\n",
      "Size: 12.0, Bold: False, Text: 'the main clause comes first:'\n",
      "Size: 9.96, Bold: False, Text: 'Phrases must not be set off by commas if this changes the intended meaning of'\n",
      "Size: 9.96, Bold: False, Text: 'the sentence.'\n",
      "Size: 12.0, Bold: False, Text: 'Nevertheless a comma is often possible, especially when the subordinate clause'\n",
      "Size: 12.0, Bold: False, Text: 'expresses a contrast:'\n",
      "Size: 9.96, Bold: False, Text: 'The committee on commas was in favour of the revised wording, whereas the'\n",
      "Size: 9.96, Bold: False, Text: 'committee on semicolons was firmly against it.'\n"
     ]
    }
   ],
   "source": [
    "# === PDF STRUCTURE INSPECTION ===\n",
    "def inspect_pdf_structure(pdf_path: str, pages_to_check: List[int] = [9, 10, 11, 12]):\n",
    "    \"\"\"\n",
    "    Prints text spans and their font properties from select pages of a PDF\n",
    "    to help identify patterns for headers.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    print(f\"--- Inspecting pages {pages_to_check} ---\")\n",
    "\n",
    "    for page_num in pages_to_check:\n",
    "        print(f\"\\n--- PAGE {page_num+1} ---\")\n",
    "        page = doc.load_page(page_num)\n",
    "        blocks = page.get_text('dict')['blocks']\n",
    "        for block in blocks:\n",
    "            if 'lines' in block:\n",
    "                for line in block['lines']:\n",
    "                    for span in line['spans']:\n",
    "                        # The 'flags' attribute is a bitmask; flag 4 (or 2^4=16) indicates bold.\n",
    "                        is_bold = (span['flags'] & 16) > 0\n",
    "                        # Round size for easier comparison\n",
    "                        font_size = round(span['size'], 2)\n",
    "                        text = span['text'].strip()\n",
    "                        if text:\n",
    "                            print(f\"Size: {font_size}, Bold: {is_bold}, Text: '{text}'\")\n",
    "    doc.close()\n",
    "\n",
    "# Run the inspection\n",
    "if __name__ == '__main__':\n",
    "    inspect_pdf_structure(PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89575c96-7974-48bd-a430-57636a1a97cd",
   "metadata": {},
   "source": [
    "Based on the inspection, I identified clear patterns for the main structural elements and codified them below.\n",
    "\n",
    "  * **Chapters and Sections**: These headers have distinct font styles. Chapter titles are size `14.04` and bold, while main section titles are `12.00` and bold. I stored these findings in the `HEADER_PROFILES` dictionary.\n",
    "  * **Parts and Subsections**: These headers could be identified by their text content. `Part I` and `Part II` follow a consistent naming convention, and subsections use a numbered format like `1.1.` or `10.2.`. I created regular expressions (`PART_REGEX` and `SUBSECTION_REGEX`) to catch these.\n",
    "\n",
    "This combination of style and pattern matching allows for a more robust method of identifying the document's hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29dcbd70-18d3-46c0-b6d2-796909b2a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HEADER PROFILES ===\n",
    "# Define the font properties of structural headers in the document.\n",
    "HEADER_PROFILES = {\n",
    "    'chapter': {'size': 14.04, 'bold': True},\n",
    "    'section': {'size': 12.00, 'bold': True},\n",
    "}\n",
    "# Regex patterns used for numbered or consistently worded headers.\n",
    "SUBSECTION_REGEX = re.compile(r'^(\\d{1,2}\\.\\d{1,2}\\.)')\n",
    "PART_REGEX = re.compile(r'^Part (I|II)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430f138-6ee1-4079-a286-a93aa87a9438",
   "metadata": {},
   "source": [
    "#### **3.3.2. Creating Chunks from Document Structure**\n",
    "\n",
    "These header profiles are the foundation for the `create_section_based_chunks` function, which works like a simple **state machine**, moving through the document page by page and block by block:\n",
    "\n",
    "1.  **State Tracking**: It keeps track of the current \"state\" using a `context` dictionary (e.g., `{'part': 'Part I', 'chapter': '2. Punctuation', ...}`).\n",
    "2.  **Header Detection**: For each text block, it checks if the block's font style or text pattern matches one of our predefined header profiles.\n",
    "3.  **Chunking on State Change**: When a new header is detected, it signals the end of the previous section. The function then saves all the text it has accumulated since the last header as a single, complete chunk. It then updates its state to the new header and begins collecting text for the next chunk.\n",
    "\n",
    "This process results in chunks that align with the document's natural flow, keeping all the text for a given section together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa2d82d-24f7-472f-8a2a-d21499fcef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION-BASED CHUNKING FUNCTION ===\n",
    "def create_section_based_chunks(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"Creates semantic chunks using a layout-aware state machine.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    # Pre-scan the document to map physical page numbers to the logical numbers in the footer.\n",
    "    page_map = {page.number: int(match.group(1)) for page in doc if (match := re.search(r'\\n(\\d{1,3})/\\d{3}\\n', page.get_text()))}\n",
    "\n",
    "    chunks = []\n",
    "    context = {'part': 'N/A', 'chapter': 'N/A', 'section': 'N/A', 'subsection': 'N/A'}\n",
    "    current_text = ''\n",
    "    start_page = 0\n",
    "\n",
    "    def save_chunk():\n",
    "        nonlocal current_text\n",
    "        if current_text.strip():\n",
    "            chunks.append({'text': current_text, 'page_number': page_map.get(start_page, start_page + 1), **context})\n",
    "        current_text = ''\n",
    "\n",
    "    # Process pages containing the main, structured content.\n",
    "    for page_num in range(6, doc.page_count):\n",
    "        if page_num == 6:\n",
    "            context.update({'part': 'N/A', 'chapter': 'Introduction', 'section': 'N/A', 'subsection': 'N/A'})\n",
    "            start_page = page_num\n",
    "\n",
    "        page = doc.load_page(page_num)\n",
    "        blocks = page.get_text('dict', sort=True)['blocks']\n",
    "        \n",
    "        for block in blocks:\n",
    "            if 'lines' not in block: continue\n",
    "            \n",
    "            block_text_content = ' '.join(s['text'] for l in block['lines'] for s in l['spans']).strip()\n",
    "            if not block_text_content or 'English Style Guide' in block_text_content or re.match(r'^\\d{1,3}/\\d{3}$', block_text_content):\n",
    "                continue\n",
    "\n",
    "            span = block['lines'][0]['spans'][0]\n",
    "            style = {'size': round(span['size'], 2), 'bold': (span['flags'] & 16) > 0}\n",
    "\n",
    "            header_type = None\n",
    "            if PART_REGEX.match(block_text_content): header_type = 'part'\n",
    "            elif style == HEADER_PROFILES['chapter']: header_type = 'chapter'\n",
    "            elif style == HEADER_PROFILES['section']: header_type = 'section'\n",
    "            elif SUBSECTION_REGEX.match(block_text_content): header_type = 'subsection'\n",
    "            \n",
    "            if header_type:\n",
    "                save_chunk()\n",
    "                start_page = page_num\n",
    "                \n",
    "                header_text = block_text_content.replace('\\n', ' ').strip()\n",
    "                if header_type == 'part': context.update({'part': header_text, 'chapter': 'N/A', 'section': 'N/A', 'subsection': 'N/A'})\n",
    "                elif header_type == 'chapter': context.update({'chapter': header_text, 'section': 'N/A', 'subsection': 'N/A'})\n",
    "                elif header_type == 'section': context.update({'section': header_text, 'subsection': 'N/A'})\n",
    "                elif header_type == 'subsection':\n",
    "                    match = SUBSECTION_REGEX.match(header_text)\n",
    "                    context.update({'subsection': match.group(0) if match else 'N/A'})\n",
    "            \n",
    "            current_text += block_text_content + '\\n'\n",
    "    \n",
    "    save_chunk()\n",
    "    doc.close()\n",
    "\n",
    "    # Finalize all created chunks with metadata.\n",
    "    tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "    final_chunks = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        cleaned_text = clean_text(chunk['text'])\n",
    "        if not cleaned_text: continue\n",
    "    \n",
    "        token_count = len(tokenizer.encode(cleaned_text))\n",
    "    \n",
    "        final_chunks.append({\n",
    "            'chunk_id': f'section_based_{i:04d}',\n",
    "            'text': cleaned_text,\n",
    "            'token_count': token_count,\n",
    "            'page_number': chunk['page_number'],\n",
    "            'method': 'section_based',\n",
    "            'part': chunk['part'],\n",
    "            'chapter': chunk['chapter'],\n",
    "            'section': chunk['section'],\n",
    "            'subsection': chunk['subsection'],\n",
    "            'source_document': SOURCE_DOCUMENT\n",
    "        })\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a27d92d-fda6-46c3-8a8f-69e30f88cbb9",
   "metadata": {},
   "source": [
    "#### **3.3.3. Handling Oversized Chunks**\n",
    "\n",
    "A challenge with this method is that some logical sections can be very long, exceeding the token limit for our embedding model. The `split_oversized_chunks` function solves this problem:\n",
    "\n",
    "1.  **Identify Large Chunks**: It iterates through the section-based chunks and checks their token count against a `max_tokens` limit (e.g., 512).\n",
    "2.  **Split if Necessary**: If a chunk is too large, it's split into smaller pieces using the same `RecursiveCharacterTextSplitter` from our baseline. Chunks that are already within the limit are left untouched.\n",
    "3.  **Preserve Metadata**: When a large chunk is split, its original metadata (Part, Chapter, Section, etc.) is copied to all the new sub-chunks. This ensures that even the smaller pieces retain their full contextual information from the document's hierarchy.\n",
    "\n",
    "This two-step process gives us the best of both worlds: chunks that are semantically grouped by section, but also guaranteed to be a manageable size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d34fc4-72a6-4dfa-bafb-cddd4f0b731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OVERSIZED CHUNK SPLITTING FUNCTION ===\n",
    "def split_oversized_chunks(chunks: List[Dict], max_tokens: int = 512, overlap: int = 50) -> List[Dict]:\n",
    "    \"\"\"Splits chunks that exceed the token limit while preserving metadata.\"\"\"\n",
    "    tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_tokens,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=lambda x: len(tokenizer.encode(x)),\n",
    "        separators=['\\n\\n', '\\n', '. ', ' ', '']\n",
    "    )\n",
    "    \n",
    "    final_split_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if chunk.get('text') and len(chunk['text']) > overlap:\n",
    "            if chunk['token_count'] <= max_tokens:\n",
    "                chunk['is_split'] = False\n",
    "                final_split_chunks.append(chunk)\n",
    "            else:\n",
    "                sub_texts = splitter.split_text(chunk['text'])\n",
    "                for i, sub_text in enumerate(sub_texts):\n",
    "                    sub_chunk = chunk.copy()\n",
    "                    cleaned_sub_text = clean_text(sub_text)\n",
    "                    sub_token_count = len(tokenizer.encode(cleaned_sub_text))\n",
    "                \n",
    "                    sub_chunk.update({\n",
    "                        'chunk_id': f\"{chunk['chunk_id']}_part{i:02d}\",\n",
    "                        'text': cleaned_sub_text,\n",
    "                        'token_count': sub_token_count,\n",
    "                        'method': 'section_based_split',\n",
    "                        'is_split': True\n",
    "                    })\n",
    "                    final_split_chunks.append(sub_chunk)\n",
    "        elif chunk.get('text'):\n",
    "            chunk['is_split'] = False\n",
    "            final_split_chunks.append(chunk)\n",
    "\n",
    "    return final_split_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9273ddb-8536-47e4-a152-1cffce8fb5e6",
   "metadata": {},
   "source": [
    "### **3.4. Executing the Chunking Pipeline**\n",
    "\n",
    "With all the functions defined, this final part of the notebook puts them into action. It's time to execute both of the chunking strategies we've designed:\n",
    "1.  **Fixed-size**: It calls `create_fixed_size_chunks` to generate the chunks for our baseline strategy and saves them to `fixed_chunks.json`.\n",
    "2.  **Section-based**: It runs the more advanced pipeline by calling `create_section_based_chunks` and then passing those results to `split_oversized_chunks`. The final, optimized chunks are saved to `section_chunks.json`.\n",
    "\n",
    "The result is two separate JSON files, each containing a clean, structured list of chunks ready for the next stage of the project: embedding and indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6746e1-4e51-4099-80c5-c2b9ad1169f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Baseline Fixed-Size Chunks ---\n",
      "✅ Created 361 fixed-size chunks -> ../data/processed/fixed_chunks.json\n",
      "\n",
      "--- Creating Final Section-Based Chunks ---\n",
      "✅ Created 593 final section-based chunks -> ../data/processed/section_chunks.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === SCRIPT EXECUTION ===\n",
    "if __name__ == '__main__':\n",
    "    # --- Process and save fixed-size chunks (Baseline) ---\n",
    "    print('--- Creating Baseline Fixed-Size Chunks ---')\n",
    "    fixed_chunks = create_fixed_size_chunks(PDF_PATH)\n",
    "    with open(FIXED_CHUNK_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(fixed_chunks, f, indent=4, ensure_ascii=False)\n",
    "    print(f'✅ Created {len(fixed_chunks)} fixed-size chunks -> {FIXED_CHUNK_PATH}\\n')\n",
    "    \n",
    "    # --- Process and save section-based chunks (Optimized) ---\n",
    "    print('--- Creating Final Section-Based Chunks ---')\n",
    "    section_chunks_raw = create_section_based_chunks(PDF_PATH)\n",
    "    final_section_chunks = split_oversized_chunks(section_chunks_raw)\n",
    "    \n",
    "    with open(SECTION_CHUNK_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_section_chunks, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    print(f'✅ Created {len(final_section_chunks)} final section-based chunks -> {SECTION_CHUNK_PATH}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32909c60-1d90-4210-b52e-2e464c0a7aa8",
   "metadata": {},
   "source": [
    "#### **3.4.1. Sanity Check**\n",
    "\n",
    "After generating hundreds of chunks, it's good practice to perform a quick sanity check. The next cell does a simple \"spot check\" on the output of the more complex, section-based strategy.\n",
    "\n",
    "It prints a few sample chunks from specific subsections. This allows for a quick visual confirmation that the text content and, more importantly, the hierarchical metadata (Part, Chapter, Section, etc.) were captured and assigned correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb18a1e-661c-4c94-b3c4-96dd88b7580c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- VERIFICATION: SAMPLE OUTPUT ---\n",
      "{\n",
      "    \"chunk_id\": \"section_based_0006\",\n",
      "    \"text\": \"2.1.   The punctuation in an English text must follow the rules and conventions for  English, which often differ from those applying to other languages. Note in  particular that:\\n\\u2666   punctuation marks in English are always \\u2013 apart from dashes (see  2.17 ) and\\nellipsis points (see  2.3 ) \\u2013 closed up to the preceding word, letter or number;\\n\\u2666   stops (. ? ! : ;) are always followed by only a single (not a double) space.\",\n",
      "    \"token_count\": 105,\n",
      "    \"page_number\": 10,\n",
      "    \"method\": \"section_based\",\n",
      "    \"part\": \"Part I\",\n",
      "    \"chapter\": \"2.   Punctuation\",\n",
      "    \"section\": \"N/A\",\n",
      "    \"subsection\": \"2.1.\",\n",
      "    \"source_document\": \"English_Style_Guide-European_Commission.pdf\",\n",
      "    \"is_split\": false\n",
      "}\n",
      "{\n",
      "    \"chunk_id\": \"section_based_0008\",\n",
      "    \"text\": \"2.2.   A full stop marks the end of a sentence. All footnotes end with a full stop. Do  not use a full stop at the end of a heading.\\nNo further full stop is required if a sentence ends with an ellipsis (\\u2026), with an  abbreviation that takes a point (e.g. \\u2018etc.\\u2019) or with a quotation complete in itself  that ends in a full stop, question mark or exclamation mark before the closing  quote:\\nWinston Churchill said: \\u2018A pessimist sees the difficulty in every opportunity; an  optimist sees the opportunity in every difficulty.\\u2019\\nTruncations (in which the end of the word is deleted) are followed by a point  (for example  Co. ,  Art. ,  Chap. ), but contractions (in which the middle of the  word is removed) are not (for example  Dr ,  Ms ,  Ltd ). See also  7.2 .\",\n",
      "    \"token_count\": 198,\n",
      "    \"page_number\": 10,\n",
      "    \"method\": \"section_based\",\n",
      "    \"part\": \"Part I\",\n",
      "    \"chapter\": \"2.   Punctuation\",\n",
      "    \"section\": \"Full stop\",\n",
      "    \"subsection\": \"2.2.\",\n",
      "    \"source_document\": \"English_Style_Guide-European_Commission.pdf\",\n",
      "    \"is_split\": false\n",
      "}\n",
      "{\n",
      "    \"chunk_id\": \"section_based_0009\",\n",
      "    \"text\": \"2.3.   Ellipsis.  An ellipsis 1  is three points indicating an omission in the text. If an  ellipsis falls at the end of a sentence, there is no final full stop. However, if  followed by another punctuation mark (e.g. question mark, colon, semicolon or  quotation mark), the punctuation mark should be closed up to the ellipsis.\\nWhen placed at the beginning of the text, it is followed by a normal space.\\nWhen replacing one or more words in the middle of a sentence, it is preceded  by a hard space 2  and followed by a normal space.\\nWhen replacing one or more words at the end of a sentence, it is preceded by a  hard space 2 .\\nThe points are not enclosed in brackets:\\n\\u2018The objectives of the Union shall be achieved \\u2026 while respecting the principle  of subsidiarity.\\u2019\\nHowever, where a line or paragraph is omitted and replaced by an ellipsis, the  ellipsis should be placed within square brackets on a separate line.\\nDo not use an ellipsis to replace or reinforce the word \\u2018etc.\\u2019\",\n",
      "    \"token_count\": 228,\n",
      "    \"page_number\": 11,\n",
      "    \"method\": \"section_based\",\n",
      "    \"part\": \"Part I\",\n",
      "    \"chapter\": \"2.   Punctuation\",\n",
      "    \"section\": \"Full stop\",\n",
      "    \"subsection\": \"2.3.\",\n",
      "    \"source_document\": \"English_Style_Guide-European_Commission.pdf\",\n",
      "    \"is_split\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# === VERIFICATION ===\n",
    "if __name__ == '__main__':\n",
    "    print('--- VERIFICATION: SAMPLE OUTPUT ---')\n",
    "    # Verify the output by printing a few key chunks.\n",
    "    target_subsections = ['2.1.', '2.2.', '2.3.']\n",
    "    found_count = 0\n",
    "    for chunk in final_section_chunks:\n",
    "        if chunk.get('subsection') in target_subsections and found_count < len(target_subsections):\n",
    "            print(json.dumps(chunk, indent=4))\n",
    "            found_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5a1b2-ad94-4291-8584-8dcb975fb8d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616e9b7-9e4a-4e08-8b96-e09314dd0d6f",
   "metadata": {},
   "source": [
    "## **4. Next Steps**\n",
    "\n",
    "This notebook has successfully processed the raw PDF source document and generated two distinct sets of structured text chunks: a simple baseline using a fixed-size strategy, and a more advanced one using a layout-aware, section-based strategy.\n",
    "\n",
    "The output is two clean JSON files (`fixed_chunks.json` and `section_chunks.json`), which are the foundation for the next stage of our RAG pipeline.\n",
    "\n",
    "In the next notebook, `02_embedding_and_vectordb_setup.ipynb`, we will:\n",
    "1.  Load these processed chunks.\n",
    "2.  Use the `BAAI/bge-large-en-v1.5` model to convert each chunk's text into a vector embedding.\n",
    "3.  Set up a Weaviate vector database to store these embeddings for efficient retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
